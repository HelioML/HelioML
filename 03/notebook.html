
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Notebook &#8212; Machine Learning, Statistics, and Data Mining for Heliophysics</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../_static/logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Differential Emission Measurements" href="../04/Differential_Emission_Measurements.html" />
    <link rel="prev" title="Enhancing SDO Images" href="Enhancing_SDO_images.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Machine Learning, Statistics, and Data Mining for Heliophysics</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Front Matter
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../FrontMatter/acknowledgements.html">
   Acknowledgements
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../FrontMatter/table_of_contents.html">
   Table of Contents
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  About This Book
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../01/whys_and_whats.html">
   What is this book and why does it exist?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../01/example_fitting_time_series.html">
   An Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../01/other_references.html">
   Where to look for more information
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 1
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../02/Predicting_Coronal_Mass_Ejections.html">
   Predicting Coronal Mass Ejections
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02/notebook.html">
   Notebook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 2
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Enhancing_SDO_images.html">
   Enhancing SDO Images
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Notebook
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="#references">
   References
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 3
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../04/Differential_Emission_Measurements.html">
   Differential Emission Measurements
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04/notebook.html">
   Notebook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 4
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../05/Scintillation_Prediction.html">
   Scintillation Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05/notebook.html">
   Notebook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 5
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../06/Spectra_of_Flaring_Active_Regions.html">
   Spectra of Flaring Active Regions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06/notebook.html">
   Notebook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 6
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../07/Detection_of_Far-Side_Solar_Active_Regions.html">
   Detection of Far-Side Solar Active Regions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../07/notebook.html">
   Notebook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 7
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../08/Automated_Detection_Of_Magnetopause_Crossings.html">
   Automated Detection of Magnetopause Crossings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../08/notebook.html">
   Notebook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Chapter 8
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../09/Forecasting_Relativistic_Electrons_Outer_Radiation_Belt.html">
   Forecasting Megaelectron‐Volt Electrons Inside Earth’s Outer Radiation Belt With Machine Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../09/notebook.html">
   Notebook
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Appendix
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../Appendix/how_to_run_this_book.html">
   How to run this book
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Appendix/how_to_run_this_book.html#running-the-book-locally">
   Running the book locally
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Appendix/how_to_run_this_book.html#running-the-book-in-the-cloud">
   Running the book in the cloud
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/03/notebook.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/HelioML/HelioML/"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/HelioML/HelioML//issues/new?title=Issue%20on%20page%20%2F03/notebook.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/HelioML/HelioML/edit/master/book/03/notebook.ipynb"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/HelioML/HelioML/master?urlpath=tree/book/03/notebook.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Notebook
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deep-convolutional-neural-networks">
     Deep convolutional neural networks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#deep-neural-networks">
       Deep neural networks
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#convolutional-neural-networks">
       Convolutional neural networks
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#activation-layers">
       Activation layers
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#general-training-process">
       General training process
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#our-architecture">
       Our architecture
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#our-training-data-and-process">
       Our training data and process
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#results">
     Results
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#validation-with-synthetic-images">
       Validation with synthetic images
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#in-the-wild">
       In the wild
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#continuum-images">
       Continuum images
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-magnetogram-example-ar-11158">
       A magnetogram example: AR 11158
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#other-general-properties">
       Other general properties
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#comparison-with-a-standard-rl-deconvolution-algorithm">
       Comparison with a standard RL deconvolution algorithm
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusions-and-future-work">
     Conclusions and future work
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="notebook">
<h1>Notebook<a class="headerlink" href="#notebook" title="Permalink to this headline">¶</a></h1>
<p><span class="math notranslate nohighlight">\(\newcommand{\arcsec}{&quot;}\)</span>
<span class="math notranslate nohighlight">\(\newcommand{\AA}{Å}\)</span></p>
<p>C. J. Díaz Baso<span class="math notranslate nohighlight">\(^{1,2}\)</span> and A. Asensio Ramos<span class="math notranslate nohighlight">\(^{1,2}\)</span></p>
<p>1 Instituto de Astrofísica de Canarias, Calle Vía Láctea, 38205 La Laguna, Tenerife, Spain<br />
2 Departamento de Astrofísica, Universidad de La Laguna, 38206 La Laguna, Tenerife, Spain</p>
<hr class="docutils" />
<p>In this chapter we will learn how to use and apply deep learning tecniques to improve the resolution of our images in a fast and robust way. We have developed a deep fully convolutional neural network which deconvolves and super-resolves continuum images and magnetograms observed with the Helioseismic and Magnetic Imager (HMI) satellite. This improvement allow us to analyze the smallest-scale events in the solar atmosphere.</p>
<p>We want to note that although almost all the examples/images are written in python, we have omitted some materials in their original format (usually large FITS files) to avoid increasing the size of this notebook.</p>
<p>The software resulted from this project is hosted in the repository https://github.com/cdiazbas/enhance, which was published in <a class="reference external" href="https://arxiv.org/pdf/1706.02933.pdf">arxiv</a> and <a class="reference external" href="https://www.aanda.org/articles/aa/pdf/2018/06/aa31344-17.pdf">A&amp;A</a> with a similar explanation. This software was developed with the python library <a class="reference external" href="https://keras.io/">keras</a>. We recommend visiting the <code class="docutils literal notranslate"><span class="pre">keras</span></code> documentation for anything related to how it works.</p>
<p><img alt="example" src="../_images/imagen.gif" />
<strong>Figure 1</strong> — Example of the software <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> applied to real solar images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Before starting we have to load some modules</span>
<span class="c1"># which will allow us the calculations</span>

<span class="c1"># Libraries:</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">congrid</span> <span class="kn">import</span> <span class="n">resample</span>
<span class="kn">import</span> <span class="nn">radialProfile</span> 
<span class="kn">from</span> <span class="nn">hmiutils</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">astropy.convolution</span> <span class="kn">import</span> <span class="n">convolve_fft</span>
<span class="kn">import</span> <span class="nn">astropy.io.fits</span> <span class="k">as</span> <span class="nn">fits</span>
<span class="kn">import</span> <span class="nn">scipy.special</span> <span class="k">as</span> <span class="nn">sp</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span>---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
&lt;ipython-input-1-f9eb17e3fdd2&gt; in &lt;module&gt;
     12 from congrid import resample
     13 import radialProfile
---&gt; 14 from hmiutils import *
     15 from astropy.convolution import convolve_fft
     16 import astropy.io.fits as fits

~/Desktop/🔥/github/mbobra/HelioML/book/03/hmiutils.py in &lt;module&gt;
     15 # import time
     16 # time0 = time.time()
---&gt; 17 from astropy.convolution import AiryDisk2DKernel
     18 # from  scipy.io import readsav
     19 from scipy import fftpack

ModuleNotFoundError: No module named &#39;astropy&#39;
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Astronomical observations from Earth are always limited by the presence of the atmosphere, which strongly disturbs
the images. An obvious (but expensive) solution to this problem is to place the telescopes in space, which produces observations without any (or limited) atmospheric aberrations. Although the observations obtained from space are not affected by atmospheric seeing, the optical properties of the instrument still limits the observations.</p>
<p>In the case of near-diffraction limited observations, the point spread function (PSF) establishes the maximum allowed
spatial resolution. The PSF typically contains two different contributions. The central core is usually dominated by the Airy diffraction pattern, a consequence of the finite and circular aperture of the telescope (plus other perturbations
on the pupil of the telescope like the spiders used to keep the secondary mirror in place). The tails of the PSF are
usually dominated by uncontrolled sources of dispersed light inside the instrument, the so-called stray light. It is known that the central core limits the spatial resolution of the observations (the smallest feature that one can see in the image), while the tails reduce the contrast of the image <a class="reference external" href="#references">(Danilovic et al. 2010)</a>. Moreover, it is important to note that knowing the PSF of any instrument is a very complicated task <a class="reference external" href="#references">(Yeo et al. 2014; Couvidat et al. 2016)</a>.</p>
<p>If the PSF is known with some precision, it is possible to apply deconvolution techniques to partially remove the perturbing effect of the telescope. The deconvolution is usually carried out with the Richardson-Lucy algorithm (RL; <a class="reference external" href="#references">Richardson 1972</a>), an iterative procedure that returns a maximum-likelihood solution to the problem.
Single-image deconvolution is usually a very ill-defined problem, in which a potentially infinite number of solutions can be compatible with the observations. Consequently, some kind of regularization has to be imposed. Typically, an early-stopping strategy in the iterative process of the RL algorithm leads to a decent output, damping the high spatial frequencies that appear in any deconvolution process. However, a maximum a-posteriori approach in which
some prior information about the image is introduced often gives much better results.</p>
<img align="right" src="https://img.purch.com/h/1400/aHR0cDovL3d3dy5zcGFjZS5jb20vaW1hZ2VzL2kvMDAwLzAzMS8wOTgvb3JpZ2luYWwvc2RvLWRldGFpbGVkLmpwZw==" width="40%"/>
<p>Fortunately, spectroscopic and spectropolarimetric observations provide multi-image observations of a field-of-view (FOV) and the deconvolution process is much better defined. This deconvolution process has been tried recently with
great success by <a class="reference external" href="#references">van Noort (2012)</a>, who also introduced a strong regularization by assuming that the Stokes profiles in every pixel have to be explained with the emerging Stokes profiles from a relatively simple model atmosphere assuming local thermodynamical equilibrium. Another solution was provided by <a class="reference external" href="#references">Ruiz Cobo &amp; Asensio Ramos (2013)</a>, who assumed that the matrix built with the Stokes profiles for all observed pixels has very low rank. In other words, it means that the Stokes profiles on the FOV can be linearly expanded with a reduced set of vectors. This method was later exploited by <a class="reference external" href="#references">Quintero Noda et al. (2015)</a> with good results. Another different approach was developed by <a class="reference external" href="#references">Asensio Ramos &amp; de la Cruz Rodríguez (2015)</a> where they used the concept of sparsity (or compressibility), which means that one can linearly expand the unknown quantities in a basis set with only a few of the elements of the basis set being active. Under the assumption of sparsity, they exploited the presence of spatial correlation on the maps of physical parameters, carrying out successful inversions and deconvolution simultaneously.</p>
<p>A great science case for the application of deconvolution and super-resolution techniques is the Helioseismic and Magnetic Imager <a class="reference external" href="#references">(HMI; Scherrer et al. 2012)</a> onboard the Solar Dynamics Observatory (SDO; <a class="reference external" href="#references">Pesnell et al. 2012</a>). HMI is a space-borne observatory that deploys full-disk images (plus a magnetogram and Dopplergram) of the Sun every 45 s (or every 720 s for a better signal-to-noise ratio). The spatial resolution of these images is  <span class="math notranslate nohighlight">\(\sim 1.1''\)</span>, with a sampling of <span class="math notranslate nohighlight">\(\sim 0.5''\)</span>/pix. In spite of the enormous advantage of having such a synoptic spatial telescope without the problematic Earth’s atmosphere, the spatial resolution is not enough to track many of the small-scale solar structures of interest. The main reason for that is the sacrifice that HMI makes to cover the full disk of the Sun in the FOV on a single sensor. We think that, in the process of pushing for the advancement of science,
it is preferable to have images with a better spatial resolution and which are already compensated for the telescope PSF.</p>
<p>Under the assumption of the linear theory of image formation, and writing images in lexicographic order (so that they are assumed to be sampled at a given resolution), the observed image can be written as:</p>
<p>\begin{equation}
\mathbf{I} = \mathbf{D} [\mathbf{P} * \mathbf{O}] + \mathbf{N},                             \tag{1}
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(\mathbf{O}\)</span> is the solar image at the entrance of the telescope, <span class="math notranslate nohighlight">\(\mathbf{P}\)</span> is a convolution matrix that simulates the effect of the PSF on the image, <span class="math notranslate nohighlight">\(\mathbf{D}\)</span> is a sub-sampling (non-square) matrix that reduces the
resolution of the input image to the desired output spatial resolution and <span class="math notranslate nohighlight">\(\mathbf{N}\)</span> represents noise (usually with Gaussian or Poisson statistics). The solution to the single-image deconvolution+super-resolution problem (SR; <a class="reference external" href="#references">Borman &amp; Stevenson 1998</a>) requires the recovery of <span class="math notranslate nohighlight">\(\mathbf{O}\)</span> (a high-resolution image of <span class="math notranslate nohighlight">\(2N \times 2N\)</span> pixels) from a single measurement <span class="math notranslate nohighlight">\(\mathbf{I}\)</span> (a low-resolution image of <span class="math notranslate nohighlight">\(N \times N\)</span> pixels). This problem is extremely ill-posed, even worse than the usual deconvolution to correct from the effect of the PSF. A multiplicity (potentially an infinite number) of solutions exists. This problem is then typically solved by imposing strong priors on the image (e.g., <a class="reference external" href="#references">Tipping &amp; Bishop 2003</a>).</p>
<p>Despite the difficulty of the problem, we think there is great interest in enhancing the HMI images using post-facto
techniques. A super-resolved image could help detect or characterize small features in the surface of the Sun, or improve
the estimation of the total magnetic flux limited by the resolution in the case of magnetograms. This motivated us to develop an end-to-end fast method based on a deep, fully convolutional neural network that simultaneously deconvolves and super-resolves the HMI continuum images and magnetograms by a factor of two. We prefer to be conservative and only do super-resolution by a factor two because our tests with a larger factor did not produce satisfactory results.
Deep learning, single-image deconvolution and super-resolution has recently been applied with great success to natural images <a class="reference external" href="#references">(Xu et al. 2014; Dong et al. 2015, 2016; Shi et al. 2016; Ledig et al. 2016; Hayat 2017)</a>. Given the variability of all possible natural images, a training-based approach should give much better results in our case than in the case of natural images. In the following, we give details about the architecture and training of the neural network and provide examples of applications to HMI data.</p>
<p><a id="fig:networks"></a>
<img align="left" src="docs/fullc.png" width="40%"/> <img align="center" src="docs/ConvLayer.png" width="50%"/></p>
<p><strong>Figure 2</strong> — Left panel: building block of a fully-connected neural network. Each input of the previous
layer is connected to each neuron of the output. Each connection is represent by different
lines where the width is associated to higher weights and the dashed lines to negative weights.
Right panel: three-dimensional convolution carried out by a convolutional layer. The 3D-kernel traverses
the whole input, producing a single scalar at each position. At the end, a 2D feature map will be
created for each 3D kernel. When all feature maps are stacked, a feature map tensor will be created.</p>
</div>
<hr class="docutils" />
<div class="section" id="deep-convolutional-neural-networks">
<h2>Deep convolutional neural networks<a class="headerlink" href="#deep-convolutional-neural-networks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="deep-neural-networks">
<h3>Deep neural networks<a class="headerlink" href="#deep-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>Artificial neural networks (ANN) are well-known computing systems based on connectionism that can be considered to be very powerful approximators to arbitrary functions <a class="reference external" href="#references">(Bishop 1996)</a>. They are constructed by putting together many basic fundamental structures (called neurons) and connecting them massively. Each neuron <span class="math notranslate nohighlight">\(i\)</span> is only able to carry out a very basic operation on the input vector: it multiplies all the input values <span class="math notranslate nohighlight">\(x_j\)</span> by some weights <span class="math notranslate nohighlight">\(w_j\)</span>, adds some bias <span class="math notranslate nohighlight">\(b_i\)</span> and finally returns the value of a certain user-defined nonlinear activation function <span class="math notranslate nohighlight">\(f(x)\)</span>. In mathematical notation, a neuron computes:</p>
<p>\begin{equation}
o_i = f(\Sigma_j,x_j\cdot w_j + b_i).              \tag{2}
\end{equation}</p>
<p>The output <span class="math notranslate nohighlight">\(o_i\)</span> is then input in another neuron that carries out a similar task. An ANN can be understood as a pipeline where the information goes from the input to the output, with each neuron making a transformation like the one described above (see left panel of Fig. <a class="reference external" href="#fig:networks">2</a>). Given that neurons are usually grouped in layers, the term deep neural network comes from the large number of layers that are used to build the neural network. Some of the most successful and recent neural networks contain several million neurons organized in several tens or hundreds of layers <a class="reference external" href="#references">(Simonyan &amp; Zisserman 2014)</a>. As a consequence, deep neural networks can be considered to be a very complex composition of very simple nonlinear functions, which provides the capacity to make very complex transformations.</p>
<p>The most used type of neural network from the 1980s to the 2000s is the fully connected network (FCN; see <a class="reference external" href="#references">Schmidhuber 2014</a>, for an overview), in which every input is connected to every neuron of the following layer. Likewise, the output transformation becomes the input of the following layer (see left panel of Fig. <a class="reference external" href="#fig:networks">2</a>).
This kind of architecture succeeded in solving problems that were considered to be not easily solvable, such as the recognition of handwritten characters <a class="reference external" href="#references">(Bishop 1996)</a>. A selection of applications in solar physics include the inversion of Stokes profiles (e.g., <a class="reference external" href="#references">Socas-Navarro 2005; Carroll &amp; Kopf 2008</a>), the acceleration of the solution of chemical equilibrium <a class="reference external" href="#references">(Asensio Ramos &amp; Socas-Navarro 2005)</a>, and the automatic classification of sunspot groups <a class="reference external" href="#references">(Colak &amp; Qahwaji 2008)</a>.</p>
<p>Neural networks are optimized iteratively by updating the weights and biases so that
a loss function that measures the ability of the network to predict the output from the input
is minimized<a class="reference external" href="#footnote1"><span class="math notranslate nohighlight">\(^{[1]}\)</span></a>. This optimization is widely known as
the learning or training process. In this process a training dataset is required.</p>
<p><a id="footnote1"></a></p>
<blockquote>
<div><p>1.- This is the case of supervised training. Unsupervised neural networks are also widespread but are of no concern in this study.</p>
</div></blockquote>
</div>
<div class="section" id="convolutional-neural-networks">
<h3>Convolutional neural networks<a class="headerlink" href="#convolutional-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>In spite of the relative success of neural networks, their application to high-dimensional objects like images or videos turned out to be an obstacle. The fundamental reason was that the number of weights in a fully connected network increases extremely fast with the complexity of the network (number of neurons) and the computation quickly becomes unfeasible. As each neuron has to be connected with the whole input, if we add a new neuron we will add the size of the input in number of weights. Then, a larger number of neurons implies a huge number of connections. This constituted an apparently unsurmountable handicap that was only solved with the appearance of convolution neural networks (CNN
or ConvNets; <a class="reference external" href="#references">LeCun &amp; Bengio 1998</a>).</p>
<p>The most important ingredient in the CNN is the convolutional layer which is composed of several convolutional neurons. Each CNN neuron carries out the convolution of the input with a certain (typically small) kernel, providing as output what is known as a feature map. Similar to a FCN, the output of convolutional neurons is often passed through a nonlinear activation function. The fundamental advantage of CNNs is that the same weights are shared across the whole input,
drastically reducing the number of unknowns. This also makes CNN shift invariant (features can be detected in an image irrespectively of where they are located).</p>
<p>In mathematical notation, for a two-dimensional input <span class="math notranslate nohighlight">\(X\)</span> of size <span class="math notranslate nohighlight">\(N \times N\)</span> with <span class="math notranslate nohighlight">\(C\)</span> channels<a class="reference external" href="#footnote2"><span class="math notranslate nohighlight">\(^{[2]}\)</span></a>
(really a cube or tensor of size <span class="math notranslate nohighlight">\(N \times N \times C\)</span>), each output feature map <span class="math notranslate nohighlight">\(O_i\)</span> (with size <span class="math notranslate nohighlight">\(N \times N \times 1\)</span>) of a convolutional layer is computed as:</p>
<p>\begin{equation}
O_i=K_i * X + b_i,               \tag{3}
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(K_i\)</span> is the <span class="math notranslate nohighlight">\(K \times K \times C\)</span> kernel tensor associated with the output feature map <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(b_i\)</span> is a bias value (<span class="math notranslate nohighlight">\(1 \times 1 \times 1\)</span>) and the convolution is displayed with the symbol <span class="math notranslate nohighlight">\(*\)</span>. Once the convolution with <span class="math notranslate nohighlight">\(M\)</span> different kernels is carried out and stacked together, the output <span class="math notranslate nohighlight">\(O\)</span> will have size <span class="math notranslate nohighlight">\(N \times N \times M\)</span>. All convolutions are here indeed intrinsically three dimensional, but one could see them as the total of <span class="math notranslate nohighlight">\(M \times C\)</span> two-dimensional convolutions plus the bias (see right panel of Fig. <a class="reference external" href="#fig:networks">2</a>).</p>
<p><a id="footnote2"></a></p>
<blockquote>
<div><p>2.- The term “channels” is inherited from those of a color image (e.g., RGB channels). However, the term has a much more general scope and can be used for arbitrary quantities (see <a class="reference external" href="#references">Asensio Ramos et al. 2017</a>, an application).</p>
</div></blockquote>
<p>CNNs are typically composed of several layers. This layered architecture exploits the property that many natural signals are generated by a hierarchical composition of patterns. For instance, faces are composed of eyes, while eyes contain a similar internal structure. This way, one can devise specific kernels that extract this information from the input. As an example, Fig. <a class="reference external" href="#fig:featuremap">3</a> shows the effect of a vertical border detection kernel on a real solar image. The result at the right of the figure is the feature map. CNNs work on the idea that each convolution layer extracts information about certain patterns, which is done during the training by iteratively adapting the set of convolutional kernels to the specific features to locate. This obviously leads to a much more optimal solution as compared with hand-crafted kernels. Despite the exponentially smaller number of free parameters as compared with a fully connected ANN, CNNs produce much better results. It is interesting to note that, since a convolutional layer simply computes sums and multiplications of the inputs, a multi-layer FCN (also known as perceptron) is perfectly capable of reproducing it, but it would require more training time (and data) to learn to approximate that mode of operation
<a class="reference external" href="#references">(Peyrard et al. 2015)</a>.</p>
<p>Although a convolutional layer significantly decreases the number of free parameters as compared with a fully connected layer, it introduces some hyperparameters (global characteristics of the network) to be set in advance: the number of kernels to be used (number of feature maps to extract from the input), size of each kernel with its corresponding padding (to deal with the borders of the image) and stride (step to be used during the convolution operation), and the number of convolutional layers and specific architecture to use in the network. As a general rule, the deeper the CNN, the better the result, at the expense of a more difficult and computationally intensive training. CNNs have recently been used in astrophysics for denoising images of galaxies <a class="reference external" href="#references">(Schawinski et al. 2017)</a>, for cosmic string detection in CMB temperature maps <a class="reference external" href="#references">(Ciuca et al. 2017)</a>, and for the estimation of horizontal velocities in the solar surface <a class="reference external" href="#references">(Asensio Ramos et al. 2017)</a> .</p>
<p><a id="fig:featuremap"></a>
<img src="docs/convolution2.png" width="90%"/></p>
<p><strong>Figure 3</strong> —  An example of a convolution with a filter. In this example, a vertical border-locating
kernel is convolved with the input image of the Sun. A resulting feature map of size
<span class="math notranslate nohighlight">\((N-2)\times(N-2)\)</span> is generated from the convolution.</p>
</div>
<div class="section" id="activation-layers">
<h3>Activation layers<a class="headerlink" href="#activation-layers" title="Permalink to this headline">¶</a></h3>
<p>As stated above, the output of a convolutional layer is often passed through a nonlinear function  that is termed the activation function. Since the convolution operation is linear, this activation is the one that introduces the nonlinear character of the CNNs. Although hyperbolic tangent, <span class="math notranslate nohighlight">\(f(x)=\tanh(x)\)</span>, or sigmoidal, <span class="math notranslate nohighlight">\(f(x)=[1+\exp(-x)]^{-1}\)</span>, activation units were originally used in ANNs, nowadays a panoply of more convenient nonlinearities are used. The main problem
with any sigmoid-type activation function is that its gradient vanishes for very large values, hindering the training of the network. Probably the most common activation function is the rectified linear unit (ReLU; <a class="reference external" href="#references">Nair &amp; Hinton 2010</a>) or slight variations of it. The ReLU replaces all negative values in the input by zero and leaves the rest untouched. This activation has the desirable property of producing nonvanishing gradients for positive arguments, which greatly accelerates the training.</p>
<blockquote>
<div><p>Note: there is a lot of other new activation functions which are less used but they have an excellent performance, like ELUs (https://arxiv.org/abs/1511.07289)</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.dpi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">150</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mf">3.5</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f(x)=tanh(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">)),</span> <span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;f(x)=1/[1+e$^{-x}$]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;f(x)=max(0,x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">ls</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">,</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/notebook_19_0.png" src="../_images/notebook_19_0.png" />
</div>
</div>
</div>
<div class="section" id="general-training-process">
<h3>General training process<a class="headerlink" href="#general-training-process" title="Permalink to this headline">¶</a></h3>
<p>CNNs are trained by iteratively modifying the weights and biases of the convolutional layers (and any other possibly learnable parameter in the activation layer). The aim is to optimize a user-defined loss function from the output
of the network and the desired output of the training data. The optimization is routinely solved using simple first-order gradient-descent algorithms (GD; see <a class="reference external" href="#references">Rumelhart et al. 1988</a>), which modifies the weights along
the negative gradient of the loss function with respect to the model parameters to carry out the update. The gradient of the loss function with respect to the free parameters of the neural network is obtained through the backpropagation algorithm <a class="reference external" href="#references">(LeCun et al. 1998)</a>. Given that neural networks are defined as a stack of modules (or layers), the gradient of the loss function can be calculated using the chain rule as the product of the gradient of each module and, ultimately, of the last layer and the specific loss function.</p>
<p>In practice, procedures based on the so-called stochastic gradient descent (SGD) are used, in which only a few examples (termed batch) from the training set are used during each iteration to compute a noisy estimation of the gradient and adjust the weights accordingly. Although the calculated gradient is a noisy estimation of the one calculated with the whole training set, the training is faster, as we have less to compute, and is more reliable. If the general loss function <span class="math notranslate nohighlight">\(Q\)</span> is the average of each loss <span class="math notranslate nohighlight">\(Q_j\)</span>  computed on a batch of inputs and  can be written as <span class="math notranslate nohighlight">\(Q=\Sigma_j^n Q_j/n\)</span>, the weights <span class="math notranslate nohighlight">\(w_i\)</span> are updated following the same recipe as the GD algorithm but calculating the gradient within a single batch:</p>
<p>\begin{equation}
w_{i+1} = w_i -\eta\nabla Q(w_i) = w_i -\eta\nabla\Sigma_j^n Q_j(w_i)/n \simeq w_i -\eta\nabla Q_j(w_i),     \tag{4}
\end{equation}
where <span class="math notranslate nohighlight">\(\eta\)</span> is the so-called learning rate. It can be kept fixed or it can be changed according to our requirements. This parameter has to be tuned to find a compromise between the accuracy of the network and the speed of convergence. If <span class="math notranslate nohighlight">\(\eta\)</span> is too large, the steps will be too large and the solution could potentially overshoot the minimum. On the contrary, if it is too small it will take too many iterations to reach the minimum. Adaptive methods like
Adam \citep{adam14} have been developed to automatically tune the
learning rate.</p>
<p>Because of the large number of free parameters in a deep CNN, overfitting can be
a problem. One would like the network to generalize well and avoid any type of “memorization” of
the training set. To check for that, a part of the training set is not used during the update
of the weights but used after each iteration as validation. Desirably, the loss should
decrease both in the training and validation sets simultaneously. If overfitting occurs, the
loss in the validation set will increase.</p>
<p>Moreover, several techniques have been described in the literature to accelerate
the training of CNNs and also to improve generalization. Batch normalization <a class="reference external" href="#references">(Ioffe &amp; Szegedy 2015)</a>
is a very convenient and easy-to-use technique that consistently produces large accelerations in the
training. It works by normalizing every batch to have
zero mean and unit variance. Mathematically, the input is normalized so that:
\begin{align}
y_i &amp;= \gamma \hat{x_i} + \beta \nonumber \
\hat{x_i} &amp;= \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}},      \tag{5}
\end{align}
where <span class="math notranslate nohighlight">\(\mu\)</span> and <span class="math notranslate nohighlight">\(\sigma\)</span> are the mean and standard deviation of the inputs on the batch and
<span class="math notranslate nohighlight">\(\epsilon=10^{-3}\)</span> is a small number to avoid underflow. The parameters <span class="math notranslate nohighlight">\(\gamma\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>
are learnable parameters that are modified during the training.</p>
<img align="right" src="https://cdn-images-1.medium.com/max/800/1*SYE7k-7o-O5qQ-4Q6u2qGw.png" width="40%"/> 
> Note: The BN adds robustness to the network. Here there is an example of the accuracy (ability of the network to predict the result) during a training with and without using BN: 
<blockquote>
<div><p>Source: https://medium.com/&#64;mozesr/batch-normalization-notes-c527c6bbec4 &amp; https://github.com/udacity/deep-learning/blob/master/batch-norm/Batch_Normalization_Lesson.ipynb</p>
</div></blockquote>
</div>
<div class="section" id="our-architecture">
<h3>Our architecture<a class="headerlink" href="#our-architecture" title="Permalink to this headline">¶</a></h3>
<p>We describe in the following the specific architecture of the two deep neural networks used to deconvolve and super-resolve continuum images and magnetograms. It could potentially be possible to use a single network to deconvolve and
super-resolve both types of images. However as each type of data has different well defined properties (like the usual range of values, or the sign of the magnitude) we have decided to use two different neural networks, finding remarkable results. We refer to the set of two deep neural networks as <code class="docutils literal notranslate"><span class="pre">Enhance</span></code>.</p>
<p>The deep neural networks used in this work are inspired by DeepVel <a class="reference external" href="#references">(Asensio Ramos et al. 2017)</a>, used to infer horizontal velocity fields in the solar photosphere. Figure <a class="reference external" href="#fig:scheme">4</a> represents a schematic view of the architecture. It is made of the concatenation of <span class="math notranslate nohighlight">\(N\)</span> residual blocks <a class="reference external" href="#references">(He et al. 2015)</a>. Each one is composed of several convolutional layers (two in our case) followed by batch normalizations and a ReLU layer for the first convolutional layer. The internal structure of a residual block is displayed in the blowup<a class="reference external" href="#footnote3"><span class="math notranslate nohighlight">\(^{[3]}\)</span></a> of Fig. <a class="reference external" href="#fig:scheme">4</a>).</p>
<p><a id="footnote3"></a></p>
<blockquote>
<div><p>3.- We note that we use the nonstandard implementation of a residual block where the second ReLU activation is removed from the reference architecture <a class="reference external" href="#references">(He et al. 2015)</a>, which provides better results according to  https://github.com/gcr/torch-residual-networks</p>
</div></blockquote>
<p>Following the typical scheme of a residual block, there is also a shortcut connection between the input and the output of the block (see more information in <a class="reference external" href="#references">He et al. 2015; Asensio Ramos et al. 2017</a>), so that the input is added to the output. Very deep networks usually saturate during training producing higher errors than shallow networks because of difficulties during training (also known as the degradation problem). The fundamental reason is that the gradient of the loss function with respect to parameters in early layers becomes exponentially small (also known as the vanishing gradient problem). Residual networks help avoid this problem obtaining state-of-the-art results without adding any extra parameter and with practically the same computational complexity. It is based on the idea that if <span class="math notranslate nohighlight">\(y=F(x)\)</span> represents the desired effect of the block on the input <span class="math notranslate nohighlight">\(x\)</span>, it is much simpler for a network to learn the deviations from the input (or residual mapping), that is <span class="math notranslate nohighlight">\(R(x)=y-x\)</span>, than the full map <span class="math notranslate nohighlight">\(F(x)\)</span>, so that <span class="math notranslate nohighlight">\(y=F(x)=R(x)+x\)</span>.</p>
<blockquote>
<div><p>Note: Here there is the diference of two neural networks of 18 and 34 layers trained without (left) and with (right) shortcuts. More information in https://arxiv.org/abs/1512.03385  <img src="docs/plainVSres1.png" width="60%"/></p>
</div></blockquote>
<p>In our case, all convolutions are carried out with kernels of size <span class="math notranslate nohighlight">\(3 \times 3\)</span> and each convolutional layer uses 64 such kernels. Additionally, as displayed in Fig. <a class="reference external" href="#fig:scheme">4</a>, we also impose another shortcut connection between the input to the first residual block and the batch normalization layer after the last residual block. We have checked that this slightly increase the quality of the prediction. Noting that a convolution of an <span class="math notranslate nohighlight">\(N \times N\)</span> image with a <span class="math notranslate nohighlight">\(3 \times 3\)</span> kernel reduces the size of the output to <span class="math notranslate nohighlight">\((N-2) \times (N-2)\)</span>, we augment the input image with 1 pixel in each side using a reflection padding to compensate for this and maintain the size of the input and output.</p>
<p>Because <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> carries out <span class="math notranslate nohighlight">\(\times 2\)</span> super-resolution, we need to add an upsampling layer somewhere in the architecture (displayed in violet in Fig. <a class="reference external" href="#fig:scheme">4</a>). One can find in the literature two main options to do the upsampling. The first one involves upsampling the image just after the input and allowing the rest of the convolutional layers to do the work. The second involves doing the upsampling immediately before the output. Following <a class="reference external" href="#references">Dong et al. (2016)</a>, we prefer the second option because it provides a much faster network, since the convolutions are applied to smaller images. Moreover, to avoid artifacts in the upsampling<a class="reference external" href="#footnote4"><span class="math notranslate nohighlight">\(^{[4]}\)</span></a> we have implemented a nearest-neighbor resize followed by convolution instead of the more standard transpose convolution.</p>
<p><a id="footnote4"></a></p>
<blockquote>
<div><p>4.- The checkerboard artifacts are nicely explained in https://distill.pub/2016/deconv-checkerboard/:</p>
</div></blockquote>
<blockquote>
<div><p><img align="left" src="https://distill.pub/2016/deconv-checkerboard/assets/style_artifacts.png" width="40%"/><img align="center" src="https://distill.pub/2016/deconv-checkerboard/assets/style_clean.png" width="40%"/></p>
</div></blockquote>
<p>The last layer that carries out a <span class="math notranslate nohighlight">\(1 \times 1\)</span> convolution is of extreme importance in our networks. Given that we use ReLU activation layers throughout the network, it is only in this very last layer where the output gets its sign using
the weights associated to the layer. This is of no importance for intensity images, but turns out to be crucial for the signed magnetic field.</p>
<p>The number of free parameters of our CNN can easily be obtained using the previous information. In the scheme of Fig. <a class="reference external" href="#fig:scheme">4</a>, the first  convolution layer generates 64 channels by applying 64 different kernels of size  <span class="math notranslate nohighlight">\(3 \times 3 \times 1\)</span> to the input (a single-channel image), using <span class="math notranslate nohighlight">\((3\times3+1)\times 64=640\)</span> free parameters. The following convolutional layers again have 64 kernel filters, but this time each one of size <span class="math notranslate nohighlight">\((3 \times 3 \times 64 +1)\)</span>, with a total of 36928 free parameters. Finally, the last layer contains one kernel of size <span class="math notranslate nohighlight">\(1 \times 1 \times 64\)</span>, that computes a weighted average along all channels. The total amount of free parameters in this layer is 65 (including the bias).</p>
<hr class="docutils" />
<p><a id="fig:scheme"></a>
<img src="docs/NETWORK2.png" width="90%"/>
<strong>Figure 4</strong> —  Upper panel: architecture of the fully convolutional neural network used in this work. Colors refer to different types of layers, which are indicated in the upper labels. The kernel size of convolutional layers are also indicated in the lower labels. Black layers represent the input and output layers. Lower panel: the inner structure of a residual block.</p>
<p>This model is hosted in https://github.com/cdiazbas/enhance</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">GaussianNoise</span><span class="p">,</span> <span class="n">add</span><span class="p">,</span> <span class="n">UpSampling2D</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.regularizers</span> <span class="kn">import</span> <span class="n">l2</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">keras.engine.topology</span> <span class="kn">import</span> <span class="n">Layer</span>
<span class="kn">from</span> <span class="nn">keras.engine</span> <span class="kn">import</span> <span class="n">InputSpec</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">conv_utils</span>
<span class="kn">from</span> <span class="nn">models</span> <span class="kn">import</span> <span class="n">ReflectionPadding2D</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Using TensorFlow backend.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You can check your version of keras doing this</span>
<span class="kn">import</span> <span class="nn">keras</span>
<span class="n">keras</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;2.1.1&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">Enhance_model</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">,</span> <span class="n">noise</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">n_filters</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Deep residual network that keeps the size of the input throughout the whole network</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Residual block definition</span>
    <span class="k">def</span> <span class="nf">residual</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">inputs</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">x</span>

    
    <span class="c1"># Inputs of the network</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">nx</span><span class="p">,</span> <span class="n">ny</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># Noise used in the training</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">GaussianNoise</span><span class="p">(</span><span class="n">noise</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x0</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">depth</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_filters</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">add</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">x0</span><span class="p">])</span>

    <span class="c1"># Upsampling for superresolution</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">UpSampling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">ReflectionPadding2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="n">n_filters</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">Activation</span><span class="p">(</span><span class="n">activation</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">final</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">final</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Using the method <code class="docutils literal notranslate"><span class="pre">summary</span></code> of the class, we can see a description of all the layers and free parameters of the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span><span class="mi">50</span> <span class="c1"># If for example the images have a size of 50 x 50</span>
<span class="n">depth</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Enhance_model</span><span class="p">(</span><span class="n">ny</span><span class="p">,</span> <span class="n">nx</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">n_filters</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 50, 50, 1)    0                                            
__________________________________________________________________________________________________
gaussian_noise_1 (GaussianNoise (None, 50, 50, 1)    0           input_1[0][0]                    
__________________________________________________________________________________________________
reflection_padding2d_1 (Reflect (None, 52, 52, 1)    0           gaussian_noise_1[0][0]           
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 50, 50, 64)   640         reflection_padding2d_1[0][0]     
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 50, 50, 64)   0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
reflection_padding2d_2 (Reflect (None, 52, 52, 64)   0           activation_1[0][0]               
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 50, 50, 64)   36928       reflection_padding2d_2[0][0]     
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 50, 50, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 50, 50, 64)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
reflection_padding2d_3 (Reflect (None, 52, 52, 64)   0           activation_2[0][0]               
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 50, 50, 64)   36928       reflection_padding2d_3[0][0]     
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 50, 50, 64)   256         conv2d_3[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 50, 50, 64)   0           batch_normalization_2[0][0]      
                                                                 activation_1[0][0]               
__________________________________________________________________________________________________
reflection_padding2d_4 (Reflect (None, 52, 52, 64)   0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 50, 50, 64)   36928       reflection_padding2d_4[0][0]     
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 50, 50, 64)   256         conv2d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 50, 50, 64)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
reflection_padding2d_5 (Reflect (None, 52, 52, 64)   0           activation_3[0][0]               
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 50, 50, 64)   36928       reflection_padding2d_5[0][0]     
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 50, 50, 64)   256         conv2d_5[0][0]                   
__________________________________________________________________________________________________
add_2 (Add)                     (None, 50, 50, 64)   0           batch_normalization_4[0][0]      
                                                                 add_1[0][0]                      
__________________________________________________________________________________________________
reflection_padding2d_6 (Reflect (None, 52, 52, 64)   0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 50, 50, 64)   36928       reflection_padding2d_6[0][0]     
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 50, 50, 64)   256         conv2d_6[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 50, 50, 64)   0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
reflection_padding2d_7 (Reflect (None, 52, 52, 64)   0           activation_4[0][0]               
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 50, 50, 64)   36928       reflection_padding2d_7[0][0]     
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 50, 50, 64)   256         conv2d_7[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 50, 50, 64)   0           batch_normalization_6[0][0]      
                                                                 add_2[0][0]                      
__________________________________________________________________________________________________
reflection_padding2d_8 (Reflect (None, 52, 52, 64)   0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 50, 50, 64)   36928       reflection_padding2d_8[0][0]     
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 50, 50, 64)   256         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 50, 50, 64)   0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
reflection_padding2d_9 (Reflect (None, 52, 52, 64)   0           activation_5[0][0]               
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 50, 50, 64)   36928       reflection_padding2d_9[0][0]     
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 50, 50, 64)   256         conv2d_9[0][0]                   
__________________________________________________________________________________________________
add_4 (Add)                     (None, 50, 50, 64)   0           batch_normalization_8[0][0]      
                                                                 add_3[0][0]                      
__________________________________________________________________________________________________
reflection_padding2d_10 (Reflec (None, 52, 52, 64)   0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 50, 50, 64)   36928       reflection_padding2d_10[0][0]    
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 50, 50, 64)   256         conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 50, 50, 64)   0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
reflection_padding2d_11 (Reflec (None, 52, 52, 64)   0           activation_6[0][0]               
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 50, 50, 64)   36928       reflection_padding2d_11[0][0]    
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 50, 50, 64)   256         conv2d_11[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 50, 50, 64)   0           batch_normalization_10[0][0]     
                                                                 add_4[0][0]                      
__________________________________________________________________________________________________
reflection_padding2d_12 (Reflec (None, 52, 52, 64)   0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 50, 50, 64)   36928       reflection_padding2d_12[0][0]    
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 50, 50, 64)   256         conv2d_12[0][0]                  
__________________________________________________________________________________________________
add_6 (Add)                     (None, 50, 50, 64)   0           batch_normalization_11[0][0]     
                                                                 activation_1[0][0]               
__________________________________________________________________________________________________
up_sampling2d_1 (UpSampling2D)  (None, 100, 100, 64) 0           add_6[0][0]                      
__________________________________________________________________________________________________
reflection_padding2d_13 (Reflec (None, 102, 102, 64) 0           up_sampling2d_1[0][0]            
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 100, 100, 64) 36928       reflection_padding2d_13[0][0]    
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 100, 100, 64) 0           conv2d_13[0][0]                  
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 100, 100, 1)  65          activation_7[0][0]               
==================================================================================================
Total params: 446,657
Trainable params: 445,249
Non-trainable params: 1,408
__________________________________________________________________________________________________
</pre></div>
</div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="our-training-data-and-process">
<h3>Our training data and process<a class="headerlink" href="#our-training-data-and-process" title="Permalink to this headline">¶</a></h3>
<p>A crucial ingredient for the success of a CNN is the generation of a suitable high-quality training set. Our network is
trained using synthetic continuum images and synthetic magnetograms from the simulation of the formation of a solar active region described by <a class="reference external" href="#references">Cheung et al. (2010)</a>. This simulation provides a large FOV with many solar-like structures (quiet Sun, plage, umbra, penumbra, etc.) that visually resemble those in the real Sun. We note that if the
network is trained properly and generalizes well, the network does not memorize what is in the training set. On the contrary, it applies what it learns to the new structures. Therefore, we are not especially concerned by the potential lack of similarity between the solar structures in the simulation of <a class="reference external" href="#references">Cheung et al. (2010)</a> and the real Sun.</p>
<p>The radiative MHD simulation was carried out with the MURaM code <a class="reference external" href="#references">(Vögler et al. 2005)</a>. The box spans 92 Mm <span class="math notranslate nohighlight">\(\times\)</span> 49 Mm in the two horizontal directions and 8.2 Mm in the vertical direction (with horizontal and vertical grid spacing of 48 and 32 km, respectively). After <span class="math notranslate nohighlight">\(\sim\)</span>20 h of solar time, an active region is formed as a consequence of the buoyancy of an injected flux tube in the convection zone. An umbra, umbral dots, light bridges, and penumbral filaments are formed during the evolution. As mentioned above, this constitutes a very nice dataset of simulated images that look very similar to those on the Sun. Synthetic gray images are generated from the simulated snapshots <a class="reference external" href="#references">(Cheung et al. 2010)</a> and magnetograms are obtained by just using the vertical magnetic field component at optical depth unity at 5000 <span class="math notranslate nohighlight">\({\AA}\)</span>. A total of 250 time steps are used in the training (slightly less for the magnetograms
when the active region has already emerged at the surface).</p>
<p>We note that the magnetograms of HMI in the Fe I 6173 <span class="math notranslate nohighlight">\({\AA}\)</span> correspond to layers in the atmosphere around log<span class="math notranslate nohighlight">\(\tau=-1\)</span> <a class="reference external" href="#references">(Bello González et al. 2009)</a>, while our magnetograms are extracted from log<span class="math notranslate nohighlight">\(\tau=0\)</span>, where <span class="math notranslate nohighlight">\(\tau\)</span> is the optical depth at 5000 <span class="math notranslate nohighlight">\({\AA}\)</span>. In our opinion this will not affect the results because the concentration of the magnetic field is similar in terms of size and shape at both atmospheric heights.</p>
<p>The synthetic images (and magnetograms) are then treated to simulate a real HMI observation. All 250 frames of 1920 <span class="math notranslate nohighlight">\(\times\)</span> 1024 images are convolved with the HMI PSF <a class="reference external" href="#references">(Wachter et al. 2012; Yeo et al. 2014; Couvidat et al. 2016)</a> and resampled to 0.504<span class="math notranslate nohighlight">\(\arcsec\)</span>/pixel. For simplicity, we have used the PSF described in <a class="reference external" href="#references">Wachter et al. (2012)</a>. The PSF functional form is azimuthally symmetric and it is given by
\begin{equation}
\mathrm{PSF}(r) = (1-\epsilon) \exp \left[ -\left(\frac{r}{\omega}\right)^2 \right] +
\epsilon \left[1+\left( \frac{r}{W}\right)^k \right]^{-1},
\end{equation}
which is a linear combination of a Gaussian and a Lorentzian. We note that the radial distance is <span class="math notranslate nohighlight">\(r=\pi D \theta/\lambda\)</span>, with <span class="math notranslate nohighlight">\(D\)</span> the telescope diameter, <span class="math notranslate nohighlight">\(\lambda\)</span> the observing wavelength and <span class="math notranslate nohighlight">\(\theta\)</span> the distance
in the focal plane in arcsec. The reference values for the parameters <a class="reference external" href="#references">(Wachter et al. 2012)</a> are <span class="math notranslate nohighlight">\(\epsilon=0.1\)</span>, <span class="math notranslate nohighlight">\(\omega=1.8\)</span>, <span class="math notranslate nohighlight">\(k=3\)</span> and <span class="math notranslate nohighlight">\(W=3\)</span>.</p>
<p>Figure <a class="reference external" href="#fig:database">5</a> demonstrates the similarity between an HMI image of the quiet Sun (upper left panel) and the simulations degraded and downsampled (lower left panel). The simulation at the original resolution is displayed in the upper right panel. For clarity, we display the horizontal and vertical axis in pixel units, instead of physical units. This reveals the difference in spatial resolution, both from the PSF convolution and the resampling. In this process we also realized that using the PSF of <a class="reference external" href="#references">Wachter et al. (2012)</a>, the azimuthally averaged power spectrum of the degraded simulated quiet Sun turns out to have stronger tails than those of the observation. For this reason, we slightly modified it so that we finally used <span class="math notranslate nohighlight">\(\omega=2\)</span> and <span class="math notranslate nohighlight">\(W=3.4\)</span>. The curve with these modified values is displayed in orange as the new PSF in Fig. <a class="reference external" href="#fig:database">5</a> with the original PSF and the default values in blue. For consistency, we also applied this PSF to the magneto-convection simulations described by <a class="reference external" href="#references">Stein &amp; Nordlund (2012)</a> and <a class="reference external" href="#references">Stein (2012)</a>, finding a similar improvement in the comparison with observations.</p>
<p>One could argue that using the more elaborate PSFs of <a class="reference external" href="#references">Yeo et al. (2014)</a> (obtained via observations of the Venus transit) or <a class="reference external" href="#references">Couvidat et al. (2016)</a> (obtained with ground data before the launch) is preferred. However, we point out that applying the PSF of <a class="reference external" href="#references">Wachter et al. (2012)</a> (with some modifications that are specified above) to the simulations produces images that compare excellently at a quantitative level with the observations. Anyway, given that our code is open sourced, anyone interested in using a different PSF can easily retrain the deep networks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here we present first a comparison between the different radial shapes of</span>
<span class="c1"># each PSF with the described parameters. We also include the Airy function</span>
<span class="c1"># as the ideal PSF of the instrument.</span>

<span class="k">def</span> <span class="nf">old_PSF</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># http://jsoc.stanford.edu/relevant_papers/Wachter_imageQ.pdf</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mf">3.0</span>
    <span class="n">w</span> <span class="o">=</span> <span class="mf">1.8</span>
    <span class="n">W</span> <span class="o">=</span> <span class="mf">3.0</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">e</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">w</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="p">)</span> <span class="o">+</span> <span class="n">e</span><span class="o">/</span><span class="p">(</span><span class="mf">1.</span><span class="o">+</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">W</span><span class="p">)</span><span class="o">**</span><span class="n">k</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">new_PSF</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="c1"># Modified version with a longer tail</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">k</span> <span class="o">=</span> <span class="mf">3.0</span>
    <span class="n">w</span> <span class="o">=</span> <span class="mf">2.0</span>
    <span class="n">W</span> <span class="o">=</span> <span class="mf">3.4</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.</span><span class="o">-</span><span class="n">e</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">w</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="p">)</span> <span class="o">+</span> <span class="n">e</span><span class="o">/</span><span class="p">(</span><span class="mf">1.</span><span class="o">+</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="n">W</span><span class="p">)</span><span class="o">**</span><span class="n">k</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">airyR</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">R</span><span class="p">):</span>
    <span class="c1"># Ideal point spread function</span>
    <span class="n">Rz</span> <span class="o">=</span> <span class="mf">1.21966989</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">R</span><span class="o">/</span><span class="n">Rz</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">sp</span><span class="o">.</span><span class="n">j1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">**</span><span class="mf">2.</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">lambdai</span> <span class="o">=</span> <span class="mf">6173e-10</span>
<span class="n">D</span> <span class="o">=</span> <span class="mf">0.14</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">D</span><span class="o">*</span><span class="n">theta</span><span class="o">/</span><span class="n">lambdai</span><span class="o">/</span><span class="mf">206265.</span>

<span class="n">res</span> <span class="o">=</span><span class="mf">1.22</span><span class="o">*</span><span class="n">lambdai</span><span class="o">/</span><span class="n">D</span><span class="o">*</span><span class="mf">206265.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Ideal resolution: </span><span class="si">{0:2.2f}</span><span class="s1"> [arcsec]</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="n">old_PSF</span><span class="p">(</span><span class="n">r</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Wachter et al. (2012)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="n">new_PSF</span><span class="p">(</span><span class="n">r</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Our work&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="n">airyR</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span><span class="n">res</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Airy function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.05</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">2.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;PSF($\theta$)&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\theta$ [arcsec]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ideal resolution: 1.11 [arcsec]
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7faf8ac79ef0&gt;
</pre></div>
</div>
<img alt="../_images/notebook_37_2.png" src="../_images/notebook_37_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here we define the 3D shape of the PSF, and not only the radial profile</span>

<span class="k">def</span> <span class="nf">new_PSF3D</span><span class="p">():</span>
    <span class="c1"># We refill the Airy PSF created by astropy with our new values</span>
    <span class="c1"># The radius of the Airy disk kernel [in pixels]</span>
    <span class="n">radio_aprx</span> <span class="o">=</span> <span class="mf">1.1</span><span class="o">/</span><span class="p">(</span><span class="mf">0.0662</span><span class="p">)</span>
    <span class="n">psfs0</span> <span class="o">=</span> <span class="n">AiryDisk2DKernel</span><span class="p">(</span><span class="n">radio_aprx</span><span class="p">)</span>

    <span class="n">psfs1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">psfs0</span><span class="p">)</span>
    <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">=</span> <span class="n">psfs0</span><span class="o">.</span><span class="n">center</span>
    <span class="k">for</span> <span class="n">ypos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">psfs1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">xpos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">psfs1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">rr</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">xpos</span><span class="o">-</span><span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="o">+</span><span class="nb">abs</span><span class="p">(</span><span class="n">ypos</span><span class="o">-</span><span class="n">y0</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="p">))</span>
            <span class="n">rr_pix</span> <span class="o">=</span> <span class="n">rr</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">D</span><span class="o">/</span><span class="n">lambdai</span><span class="o">/</span><span class="mf">206265.</span><span class="o">*</span><span class="mf">0.0662</span>
            <span class="n">psfs1</span><span class="p">[</span><span class="n">ypos</span><span class="p">,</span><span class="n">xpos</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_PSF</span><span class="p">(</span><span class="n">rr_pix</span><span class="p">)</span>
    <span class="n">psfs1</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">psfs1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">psfs1</span>

<span class="k">def</span> <span class="nf">old_PSF3D</span><span class="p">():</span>
    <span class="c1"># We refill the Airy PSF created by astropy with our new values</span>
    <span class="c1"># The radius of the Airy disk kernel [in pixels]</span>
    <span class="n">radio_aprx</span> <span class="o">=</span> <span class="mf">1.1</span><span class="o">/</span><span class="p">(</span><span class="mf">0.0662</span><span class="p">)</span>
    <span class="n">psfs0</span> <span class="o">=</span> <span class="n">AiryDisk2DKernel</span><span class="p">(</span><span class="n">radio_aprx</span><span class="p">)</span>

    <span class="n">psfs1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">psfs0</span><span class="p">)</span>
    <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">=</span> <span class="n">psfs0</span><span class="o">.</span><span class="n">center</span>
    <span class="k">for</span> <span class="n">ypos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">psfs1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">xpos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">psfs1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">rr</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">xpos</span><span class="o">-</span><span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="o">+</span><span class="nb">abs</span><span class="p">(</span><span class="n">ypos</span><span class="o">-</span><span class="n">y0</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="p">))</span>
            <span class="n">rr_pix</span> <span class="o">=</span> <span class="n">rr</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">D</span><span class="o">/</span><span class="n">lambdai</span><span class="o">/</span><span class="mf">206265.</span><span class="o">*</span><span class="mf">0.0662</span>
            <span class="n">psfs1</span><span class="p">[</span><span class="n">ypos</span><span class="p">,</span><span class="n">xpos</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_PSF</span><span class="p">(</span><span class="n">rr_pix</span><span class="p">)</span>
    <span class="n">psfs1</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">psfs1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">psfs1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We load two images of QS from the simulation and from the HMI satellite</span>
<span class="n">imSIMU</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;simulation.npy&#39;</span><span class="p">);</span> <span class="n">imHMI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;hmi.npy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">dx</span> <span class="o">=</span> <span class="mi">108</span> <span class="c1"># Size of the sample</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">221</span><span class="p">)</span>
<span class="n">pHMI</span> <span class="o">=</span> <span class="n">imHMI</span><span class="p">[:</span><span class="n">dx</span><span class="p">,:</span><span class="n">dx</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;HMI - Observation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pHMI</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;bicubic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y [pixel]&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">222</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simulation - Original&#39;</span><span class="p">)</span>
<span class="n">pSIMU</span> <span class="o">=</span> <span class="n">imSIMU</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">dx</span><span class="o">/</span><span class="mf">0.0662</span><span class="o">*</span><span class="mf">0.504</span><span class="p">),:</span><span class="nb">int</span><span class="p">(</span><span class="n">dx</span><span class="o">/</span><span class="mf">0.0662</span><span class="o">*</span><span class="mf">0.504</span><span class="p">)]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pSIMU</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;bicubic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y [pixel]&#39;</span><span class="p">)</span>
<span class="n">yticki</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">600</span><span class="p">,</span><span class="mi">800</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">yticki</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">yticki</span><span class="p">)</span>


<span class="c1"># We convolve both PSFs to compare later the images</span>
<span class="n">new_SIMU</span> <span class="o">=</span> <span class="n">convolve_fft</span><span class="p">(</span><span class="n">pSIMU</span><span class="p">,</span><span class="n">new_PSF3D</span><span class="p">(),</span><span class="n">boundary</span><span class="o">=</span><span class="s1">&#39;wrap&#39;</span><span class="p">)</span>
<span class="n">old_SIMU</span> <span class="o">=</span> <span class="n">convolve_fft</span><span class="p">(</span><span class="n">pSIMU</span><span class="p">,</span><span class="n">old_PSF3D</span><span class="p">(),</span><span class="n">boundary</span><span class="o">=</span><span class="s1">&#39;wrap&#39;</span><span class="p">)</span>

<span class="c1"># Now we resample the original images to the HMI sampling</span>
<span class="n">pnew_SIMU</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span><span class="n">new_SIMU</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">dx</span><span class="o">/</span><span class="mf">0.0662</span><span class="o">*</span><span class="mf">0.504</span><span class="p">),:</span><span class="nb">int</span><span class="p">(</span><span class="n">dx</span><span class="o">/</span><span class="mf">0.0662</span><span class="o">*</span><span class="mf">0.504</span><span class="p">)],[</span><span class="n">dx</span><span class="p">,</span><span class="n">dx</span><span class="p">])</span>
<span class="n">pold_SIMU</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span><span class="n">old_SIMU</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">dx</span><span class="o">/</span><span class="mf">0.0662</span><span class="o">*</span><span class="mf">0.504</span><span class="p">),:</span><span class="nb">int</span><span class="p">(</span><span class="n">dx</span><span class="o">/</span><span class="mf">0.0662</span><span class="o">*</span><span class="mf">0.504</span><span class="p">)],[</span><span class="n">dx</span><span class="p">,</span><span class="n">dx</span><span class="p">])</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">223</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simulation - Degraded&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pnew_SIMU</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;bicubic&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y [pixel]&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
<span class="c1"># We calculate the FFT of each image to a better comparison</span>
<span class="n">v</span><span class="p">,</span> <span class="n">psf1D</span> <span class="o">=</span> <span class="n">fft1D</span><span class="p">(</span><span class="n">pHMI</span><span class="p">)</span>
<span class="n">v2</span><span class="p">,</span> <span class="n">psf1D2</span> <span class="o">=</span> <span class="n">fft1D</span><span class="p">(</span><span class="n">pnew_SIMU</span><span class="p">)</span>
<span class="n">v3</span><span class="p">,</span> <span class="n">psf1D3</span> <span class="o">=</span> <span class="n">fft1D</span><span class="p">(</span><span class="n">pold_SIMU</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="n">psf1D</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;HMI data&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">v2</span><span class="p">,</span><span class="n">psf1D2</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;New PSF - Our work&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">v3</span><span class="p">,</span><span class="n">psf1D3</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Old PSF - Wachter et al. (2012)&#39;</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Power Spectrum of the image&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\nu$ [pix$^{-1}$]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$P(\nu)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/notebook_39_0.png" src="../_images/notebook_39_0.png" />
</div>
</div>
<p><a id="fig:database"></a>
<strong>Figure 5</strong> — Upper left: HMI observation. Upper right: snapshot from the simulation used for training. Lower left: degraded simulations, which can be compared with the HMI observations. Lower right: azimuthally averaged power spectrum of the HMI observations and the degraded simulations with the original PSF and the one modified and used in the training process. The physical dimension of the three maps is 54<span class="math notranslate nohighlight">\(\arcsec\)</span><span class="math notranslate nohighlight">\(\times\)</span>54<span class="math notranslate nohighlight">\(\arcsec\)</span>.</p>
<hr class="docutils" />
<p>Then, we randomly extract 50000 patches of <span class="math notranslate nohighlight">\(50\times 50\)</span> pixels both spatially and temporally, which will constitute the input patches of the training set. We also randomly extract a smaller subset of 5000 patches which will act as a validation set to avoid overfitting. These are used during the training to check that the CNN generalizes well and is not memorizing the training set. The targets of the training set are obtained similarly but convolving with the Airy function of a telescope twice the diameter of HMI (28 cm), which gives a diffraction limit of <span class="math notranslate nohighlight">\(0.55&quot;\)</span>/pixel, and then resampled to <span class="math notranslate nohighlight">\(0.25&quot;\)</span>/pixel. Therefore, the sizes of the output patches are <span class="math notranslate nohighlight">\(100 \times 100\)</span> pixels. All inputs and outputs for the continuum images are normalized to the average intensity of the quiet Sun. This is very convenient when the network is
deployed in production because this quantity <span class="math notranslate nohighlight">\(I/Ic\)</span> is almost always available. On the contrary, the magnetograms are divided by 10<span class="math notranslate nohighlight">\(^3\)</span>, so they are treated in kG during the training. The training of the network is carried out by minimizing a loss function defined as the squared difference between the output of the network and the desired output defined on the training set. To this end, we use the Adam stochastic optimizer <a class="reference external" href="#references">(Kingma &amp; Ba 2014)</a> with a learning rate of <span class="math notranslate nohighlight">\(\eta=10^{-4}\)</span>. The training is done in a Titan X GPU for 20 epochs, taking <span class="math notranslate nohighlight">\(\sim 500\)</span> seconds per epoch. We augment the loss function with an <span class="math notranslate nohighlight">\(\ell_2\)</span> regularization for the elements of the kernels of all convolutional layers to avoid overfitting. Finally, we add Gaussian noise (with an amplitude of 10<span class="math notranslate nohighlight">\(^{-3}\)</span> in units of the continuum intensity for the continuum images and 10<span class="math notranslate nohighlight">\(^{-2}\)</span> for the magnetograms, following <a class="reference external" href="http://hmi.stanford.edu/Description/HMI_Overview.pdf">HMI standard specifications specifications</a>) to stabilize the training and produce better quality predictions. This is important for regions of low contrast in the continuum images and regions of weak magnetic fields in the magnetograms.</p>
<p>Apart from the size and number of kernels, there are a few additional hyperparameters that need to be defined in <code class="docutils literal notranslate"><span class="pre">Enhance</span></code>. The most important ones are the number of residual blocks, the learning rate of the Adam optimizer and the amount of regularization. We have found stable training behavior with a learning rate of <span class="math notranslate nohighlight">\(10^{-4}\)</span> so we have kept this fixed. Additionally, we found that a regularization weight of <span class="math notranslate nohighlight">\(10^{-6}\)</span> for the continuum images and <span class="math notranslate nohighlight">\(10^{-5}\)</span> for the magnetograms provides nice and stable results.</p>
<p>Finally, five residual blocks with <span class="math notranslate nohighlight">\(\sim\)</span>450k free parameters provide predictions that are almost identical to those of 10 and 15 residual blocks, but much faster. We note that the number of residual blocks can be further decreased even down to one and a good behavior is still found (even if the number of kernels is decreased to 32). This version of <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> is six times faster than the one presented here, reducing the number of parameters to <span class="math notranslate nohighlight">\(\sim\)</span>40k, with differences
around 3%. Although <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> is already very fast, this simplified version can be used for an in-browser online super-resolution and deconvolution of HMI data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is an pseudocode example of the training process using keras. See the full version in the repository:</span>
<span class="c1"># https://github.com/cdiazbas/enhance/blob/master/train.py</span>

<span class="c1"># We asign the loss function and the optimizer with the learning rate to the model class</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>

<span class="c1"># And then we start the training process with the described data. Usually the method fit() is used </span>
<span class="c1"># when the dataset can be allocated in memory and fit_generator() for larger datasets.</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">validation_data</span><span class="p">)</span>

<span class="c1"># After this last process the NN can be used to do predictions in the same way:</span>
<span class="n">prediction_data</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<div class="section" id="validation-with-synthetic-images">
<h3>Validation with synthetic images<a class="headerlink" href="#validation-with-synthetic-images" title="Permalink to this headline">¶</a></h3>
<p>Before proceeding to applying the networks to real data, we show in Fig. <a class="reference external" href="#fig:validation_synthetic">6</a> the results with some of the patches from the validation set which are not used during the training. The upper three rows show results for the continuum images, while the lower three rows show results for the magnetograms. The leftmost column is the original synthetic image at the resolution of HMI. The rightmost column is the target that should be recovered by the network, which has doubled the number of pixels in each dimension. The middle column displays our single-image superresolution results.</p>
<p>Even though the appearance of all small-scale details are not exactly similar to the target, we consider that <code class="docutils literal notranslate"><span class="pre">Enhance</span></code>  is doing a very good job in deconvolving and super-resolving the data in the first column. In the regions of increased activity, we find that we are able to greatly improve the fine structure, specially in the penumbra. Many details are barely visible in the synthetic HMI image but can be guessed. Of special relevance are the protrusions in the umbra in the third row, which are very well recovered by the neural network. The network also does a very good job in the quiet Sun, correctly recovering the expected shape of the granules from the blobby appearance in
the HMI images.</p>
<p><a id="fig:validation_synthetic"></a>
<img src="docs/validation_cont.png" width="70%"/><img src="docs/validation_blos.png" width="70%"/></p>
<p><strong>Figure 7</strong> — Upper left: HMI observation. Upper right: snapshot from the simulation used for training. Lower left: degraded simulations, which can be compared with the HMI observations. Lower right: azimuthally averaged power spectrum of the HMI observations and the degraded simulations with the original PSF and the one modified and used in the training process. The physical dimension of the three maps is 54<span class="math notranslate nohighlight">\(\arcsec\)</span><span class="math notranslate nohighlight">\(\times\)</span>54<span class="math notranslate nohighlight">\(\arcsec\)</span>.</p>
</div>
<div class="section" id="in-the-wild">
<h3>In the wild<a class="headerlink" href="#in-the-wild" title="Permalink to this headline">¶</a></h3>
<p>The trained networks are then applied to real HMI data. In order to validate the output of our neural network we have selected observations of the Broadband Filter Instrument (BFI) from the Solar Optical Telescope <a class="reference external" href="#references">(SOT, Ichimoto et al. 2008; Tsuneta et al. 2008)</a> onboard Hinode <a class="reference external" href="#references">(Kosugi et al. 2007)</a>. The pixel size of the BFI is <span class="math notranslate nohighlight">\(0.109&quot;\)</span> and the selected observations were obtained in the red continuum filter at <span class="math notranslate nohighlight">\(6684 \pm 2\)</span> <span class="math notranslate nohighlight">\({\AA}\)</span>, which is the one closer to the observing wavelength of HMI. To properly compare our results with Hinode, we have convolved the BFI images with an Airy function of a telescope of 28 cm diameter and resampled to <span class="math notranslate nohighlight">\(0.25&quot;\)</span>/pixel to match those of the output of <code class="docutils literal notranslate"><span class="pre">Enhance</span></code>. The Hinode images have not been deconvolved from the influence of its PSF. We point out that the long tails of the PSF of the Hinode/SOT instrument produce a slight decrease in the contrast <a class="reference external" href="#references">(Danilovic et al. 2010)</a>  and this is the reason why our enhanced images have a larger contrast.</p>
</div>
<div class="section" id="continuum-images">
<h3>Continuum images<a class="headerlink" href="#continuum-images" title="Permalink to this headline">¶</a></h3>
<p>Figure <a class="reference external" href="#fig:subplot">7</a> displays this comparison for two different regions (columns) observed simultaneously
with Hinode and HMI. These two active regions are: NOAA 11330 (N09, E04) observed on October 27, 2011
(first column), and NOAA 12192 (S14, E05) observed on October 22, 2014 (second column). We have used HMI images with a cadence of 45 seconds, which is the worst scenario in terms of noise in the image. The upper rows show the original HMI images. The lower rows display the degraded Hinode images, while the central row shows the output of our neural network. Given the fully convolutional character of the deep neural network used in this work, it can be applied seamlessly to input images of arbitrary size. As an example, an image of size <span class="math notranslate nohighlight">\(400 \times 400\)</span> can be super-resolved and deconvolved in <span class="math notranslate nohighlight">\(\sim\)</span>100 ms using a Titan X GPU, or <span class="math notranslate nohighlight">\(\sim\)</span>1 s using a 3.4 GHz Intel Core i7.</p>
<p><a id="fig:subplot"></a>
<img src="docs/column34.png" width="80%"/>
<strong>Figure 7</strong> — Application of the neural network to real HMI images. From the upper to the lower part of each column: the original HMI images, the output of the neural network, and the degraded Hinode image. All the axes are in pixel units.</p>
<p>The contrast <span class="math notranslate nohighlight">\(\sigma_I/I\)</span>, calculated as the standard deviation of the continuum intensity divided by the
average intensity of the area, is quoted in the title of each panel and has been obtained in a small region of the image
displaying only granulation. The granulation contrast increases from <span class="math notranslate nohighlight">\(\sim\)</span>3.7% to <span class="math notranslate nohighlight">\(\sim\)</span>7% <a class="reference external" href="#references">(as Couvidat et al. 2016)</a>, almost a factor two larger than the one provided by degraded Hinode. We note that the contrast may be slightly off for the right column because of the small quiet Sun area available. The granulation contrast measured in
Hinode without degradation is around 7%. After the resampling, it goes down to the values quoted in the figure. We note that <a class="reference external" href="#references">(Danilovic et al. 2008)</a> analyzed the Hinode granulation contrast at 630 nm and concluded that it is
consistent with those predicted by the simulations (in the range 14<span class="math notranslate nohighlight">\(-\)</span>15%) once the PSF is taken into account. From a visual point of view, it is clear that <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> produces small-scale structures that are almost absent in the HMI images
but clearly present in the Hinode images. Additionally, the deconvolved and super-resolved umbra intensity decreases between 3 and 7% when compared to the original HMI umbral intensity. Interesting cases are the large light bridge in the images of the right column, that increases in spatial complexity. Another example is the regions around the light bridge, which are plagued with small weak umbral dots that are evident in Hinode data but completely smeared out in HMI. For instance, the region connecting the light bridge at (125, 240) with the penumbra. Another similar instance of this enhancement occurs (375, 190); a pore with some umbral dots that are almost absent in the HMI images.</p>
<p>As a caveat, we warn the users that the predictions of the neural network in areas close to the limb is poorer than those at disk center. Given that <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> was trained with images close to disk center, one could be tempted to think that a lack of generalization is the cause for the failure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here we describe how to use ENHANCE</span>
<span class="c1"># using a region close to the limb as an example:</span>
<span class="n">mymap</span> <span class="o">=</span> <span class="n">fits</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;hmi_20111102_000029_continuum.fits&#39;</span><span class="p">)</span>
<span class="n">dat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">mymap</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We only want a small region</span>
<span class="n">submap</span> <span class="o">=</span> <span class="n">dat</span><span class="p">[</span><span class="mi">1700</span><span class="p">:</span><span class="mi">2100</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">500</span><span class="p">]</span>
<span class="c1"># and it has to be normalized to the QS.</span>
<span class="n">maxim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dat</span><span class="p">[</span><span class="mi">0</span><span class="p">:,</span><span class="mi">0</span><span class="p">:])</span>
<span class="n">submap</span> <span class="o">=</span> <span class="n">submap</span><span class="o">/</span><span class="n">maxim</span>
<span class="c1">#submap = submap/np.mean(submap[:,400:])</span>


<span class="c1"># The image have to be save in fits format</span>
<span class="n">hdu</span> <span class="o">=</span> <span class="n">fits</span><span class="o">.</span><span class="n">PrimaryHDU</span><span class="p">(</span><span class="n">submap</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s1">&#39;rm samples/nhmi.fits&#39;</span><span class="p">)</span>
<span class="n">hdu</span><span class="o">.</span><span class="n">writeto</span><span class="p">(</span><span class="s1">&#39;samples/nhmi.fits&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Then, we run our code as it is explained in the repository:</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run</span> <span class="n">enhance</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">i</span> <span class="n">samples</span><span class="o">/</span><span class="n">nhmi</span><span class="o">.</span><span class="n">fits</span> <span class="o">-</span><span class="n">t</span> <span class="n">intensity</span> <span class="o">-</span><span class="n">o</span> <span class="n">output</span><span class="o">/</span><span class="n">hmi_enhanced</span><span class="o">.</span><span class="n">fits</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model : intensity
Setting up network...
WARNING:tensorflow:From /usr/pkg/python/Python-3.4.3/lib/python3.4/site-packages/keras/backend/tensorflow_backend.py:1242: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
Loading weights...
Predicting validation data...
Prediction took 5.7 seconds...
Saving data...
Overwriting...
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Figure size 900x600 with 0 Axes&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">mymap2</span> <span class="o">=</span> <span class="n">fits</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;output/hmi_enhanced.fits&#39;</span><span class="p">)</span>
<span class="n">submap2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">mymap2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;HMI - Enhanced&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">submap2</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">400</span><span class="p">,:],</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="n">submap2</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span><span class="n">vmax</span><span class="o">=</span><span class="n">submap2</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y [pixel]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">locator_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;HMI - Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">submap</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">200</span><span class="p">,:],</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">,</span><span class="n">vmin</span><span class="o">=</span><span class="n">submap2</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span><span class="n">vmax</span><span class="o">=</span><span class="n">submap2</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span><span class="n">labelbottom</span><span class="o">=</span><span class="s1">&#39;off&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y [pixel]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">locator_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/notebook_54_01.png" src="../_images/notebook_54_01.png" />
</div>
</div>
<p>However, we note that structures seen in the limb, such as elongated granules, share some similarity to some penumbral filaments, so these cases are already present in the training set.
The fundamental reason for the failure is that the spatial contrast in the limb is very small, so the neural network does not know how to reconstruct the structures, thus creating artifacts. We speculate that these artifacts will not be significantly reduced even if limb synthetic observations are included in the training set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;HMI - Enhanced&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">submap2</span><span class="p">[</span><span class="mi">200</span><span class="p">:</span><span class="mi">400</span><span class="p">,</span><span class="mi">200</span><span class="p">:</span><span class="mi">400</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">submap2</span><span class="p">[</span><span class="mi">200</span><span class="p">:</span><span class="mi">400</span><span class="p">,</span><span class="mi">200</span><span class="p">:</span><span class="mi">400</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">locator_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">nbins</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;HMI - Original&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">submap</span><span class="p">[</span><span class="mi">100</span><span class="p">:</span><span class="mi">200</span><span class="p">,</span><span class="mi">100</span><span class="p">:</span><span class="mi">200</span><span class="p">],</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">submap2</span><span class="p">[</span><span class="mi">200</span><span class="p">:</span><span class="mi">400</span><span class="p">,</span><span class="mi">200</span><span class="p">:</span><span class="mi">400</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y [pixel]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/notebook_56_0.png" src="../_images/notebook_56_0.png" />
</div>
</div>
</div>
<div class="section" id="a-magnetogram-example-ar-11158">
<h3>A magnetogram example: AR 11158<a class="headerlink" href="#a-magnetogram-example-ar-11158" title="Permalink to this headline">¶</a></h3>
<p>As a final example, we show in Fig. <a class="reference external" href="#fig:bigplot">8</a> an example of the neural network applied to the intensity and the magnetogram for the same region: the NOAA 11158 (S21, W28), observed on February 15, 2011. The FOV is divided into two halfs. The upper parts show the HMI original image both for the continuum image (left panel) and the magnetogram (right panel). The lower parts display the enhanced images after applying the neural network.</p>
<blockquote>
<div><p>Note: This active region has been studied in the past. See for example: http://iopscience.iop.org/article/10.1088/0004-637X/783/2/98/pdf</p>
</div></blockquote>
<p><a id="fig:bigplot"></a>
<img src="docs/figure7b.png" width="100%"/>
<strong>Figure 8</strong> — An example of our neural network applied to the intensity (left) and magnetogram (right) for the same region. The FOV is divided into two halves. The upper half shows the HMI original image, without applying the neural network. The lower half shows enhanced image applying the neural network to the last image. The original image was resampled to have the same scale as the network output.</p>
<hr class="docutils" />
<p>After the deconvolution of the magnetogram, we find that: i) regions with very nearby opposite polarities suffer from an apparent cancellation in HMI data that can be restored with <code class="docutils literal notranslate"><span class="pre">Enhance</span></code>, giving rise to an increase in the
absolute value of the longitudinal field; and ii) regions far from magnetized areas become contaminated by the surroundings in HMI, which are also compensated for with <code class="docutils literal notranslate"><span class="pre">Enhance</span></code>, returning smaller longitudinal
fields. The left panel of Fig. <a class="reference external" href="#fig:powerBlos">9</a> shows the density plot of the input versus output longitudinal magnetic field. Almost all the points lie in the 1:1 relation. However, points around 1 kG for HMI are
promoted to larger absolute values, a factor <span class="math notranslate nohighlight">\(\sim 1.3 - 1.4\)</span> higher than the original image <a class="reference external" href="#references">(Couvidat et al. 2016)</a>.</p>
<p>Another interesting point to study is the range of spatial scales at which <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> is adding information. The right panel of Fig. <a class="reference external" href="#fig:powerBlos">9</a> displays the power spectrum of both magnetograms showed in the right part of Fig. <a class="reference external" href="#fig:bigplot">8</a>. The main difference between both curves is situated in the range of spatial scales <span class="math notranslate nohighlight">\(\nu = 0.05-0.25\)</span> pix<span class="math notranslate nohighlight">\(^{-1}\)</span> with a peak at <span class="math notranslate nohighlight">\(\nu=0.15\)</span> pix<span class="math notranslate nohighlight">\(^{-1}\)</span>. In other words, the neural network is operating mainly at scales between 4 and 20 pixels, where the smearing effect of the PSF is higher.</p>
<p>The same effect can be seen when a standard Richardson–Lucy maximum-likelihood algorithm (RL) (including a bilinear interpolation to carry out the super-resolution) is used (see <a class="reference external" href="#sec:RL">next section</a> for more details). The power spectrum of the output of <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> and the one deconvolved with RL are almost the same for frequencies below 0.15 pix<span class="math notranslate nohighlight">\(^{-1}\)</span> (equivalent to scales above <span class="math notranslate nohighlight">\(\sim 6\)</span> pix). For larger frequencies (smaller scales), the RL version adds noisy
small scale structures at a level of <span class="math notranslate nohighlight">\(\sim\)</span>80 G; this is not case with <code class="docutils literal notranslate"><span class="pre">Enhance</span></code>. We note that the original image has a noise around <span class="math notranslate nohighlight">\(\sim\)</span>10 G. To quantify this last point, we have showed in Fig. <a class="reference external" href="#fig:powerBlos">9</a> the flat spectrum of white noise artificial images with zero mean and standard deviations <span class="math notranslate nohighlight">\(\sigma=12\)</span>G and <span class="math notranslate nohighlight">\(\sigma=75\)</span>G.</p>
<p><a id="fig:powerBlos"></a>
<img src="docs/2plots.png" width="100%"/>
<strong>Figure 9</strong> — Left: scatter plot between the original magnetogram signal and the deconvolved magnetogram signal. Dotted lines indicate a change of a factor two. Right: spatial Fourier power spectrum of all considered magnetograms: the original, the output of Enhance and the one deconvolved with RL. We also show the power spectrum of white noise at two different levels.</p>
</div>
<hr class="docutils" />
<div class="section" id="other-general-properties">
<h3>Other general properties<a class="headerlink" href="#other-general-properties" title="Permalink to this headline">¶</a></h3>
<p>Depending on the type of structure analyzed, the effect of the deconvolution is different. In plage regions, where the magnetic areas are less clustered than in a sunspot, the impact of the stray light is higher. Then, <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> produces a magnetic field that can increase up to a factor two <a class="reference external" href="#references">(Yeo et al. 2014)</a>, with magnetic structures smaller in size, as signal smeared onto the surrounding quiet Sun is returned to its original location. According to the left panel of Fig <a class="reference external" href="#fig:powerBlos">9</a>, fields with smaller amplitudes suffer a larger relative change. As a guide to the eye, the two dotted lines indicate a change of a factor two in the same figure.</p>
<p>To check these conclusions, we have used a Hinode-SOT Spectropolarimeter (SP) <a class="reference external" href="#references">(Lites et al. 2013)</a> Level 1D<a class="reference external" href="#footnote_sot"><span class="math notranslate nohighlight">\(^{[5]}\)</span></a> magnetogram. The region was observed in April 25, 2015, at 04:00h UT and its pixel size is around 0.30’’/pix. Figure <a class="reference external" href="#fig:compararPlage">10</a> shows the increase of the magnetic field after the deconvolution: magnetic fields of kG flux were diluted by the PSF and recovered with <code class="docutils literal notranslate"><span class="pre">Enhance</span></code>. It was impossible to find the Hinode map of exactly the same region at exactly the same moment, meaning that some differences are visible. However the general details are retrieved. In regions of strong concentrations, like the ones found in Fig. <a class="reference external" href="#fig:bigplot">8</a>, almost each polarity is spatially concentrated and increased by a factor below 1.5.</p>
<p><a id="footnote_sot"></a></p>
<blockquote>
<div><p>5.- Hinode-SOT Spectropolarimeter(SP) Data Product Description and Access: http://sot.lmsal.com/data/sot/level1d/</p>
</div></blockquote>
<p>The magnetogram case is more complex than the intensity map. Many studies <a class="reference external" href="#references">(Krivova &amp; Solanki 2004; Pietarila et al. 2013; Bamba et al. 2014)</a> have demonstrated the impact of the resolution to estimate a value of the magnetic flux and products of the magnetogram as nonlinear force-free extrapolations <a class="reference external" href="#references">(Tadesse et al. 2013; DeRosa et al. 2015)</a>, to compare with in–situ spacecraft measurements <a class="reference external" href="#references">(Linker et al. 2017)</a>.</p>
<p>Contrary to deconvolving intensity images, deconvolving magnetograms is always a very delicate issue. The difficulty relies on the presence of cancellation produced during the smearing with a PSF if magnetic elements of opposite polarities are located nearby. This never happens for intensity images, which are always non-negative. Consequently, one can arbitrarily increase the value of nearby positive and negative polarities while maintaining a small quadratic approximation to the desired output. This effect is typically seen when a standard RL algorithm is used for
deconvolution.</p>
<p><code class="docutils literal notranslate"><span class="pre">Enhance</span></code> avoids this effect by learning suitable spatial priors from the training dataset. It is true that the method will not be able to separate back two very nearby opposite polarities that have been fully canceled by the smearing of the PSF. Extensive tests show that the total absolute flux of each deconvolved image is almost the same as that
in the original image, that is, the magnetic field is mainly “reallocated”.</p>
<p><a id="fig:compararPlage"></a>
<img src="docs/compararPlage.png" width="100%"/>
<strong>Figure 10</strong> — Left: original HMI magnetogram of a plage region observed on April 25, 2015. Middle: the result of applying Enhance to the HMI magnetogram. Right: the Hinode magnetogram at the same resolution of Enhance. The magnetic flux has been clipped from −1kG to 1kG.</p>
<hr class="docutils" />
<p><a id="sec:RL"></a></p>
</div>
<div class="section" id="comparison-with-a-standard-rl-deconvolution-algorithm">
<h3>Comparison with a standard RL deconvolution algorithm<a class="headerlink" href="#comparison-with-a-standard-rl-deconvolution-algorithm" title="Permalink to this headline">¶</a></h3>
<p>As a final step, we compare our results with those of a RL algorithm in a complicated case. Figure <a class="reference external" href="#fig:compara2">11</a> shows the same image deconvolved with both methods. The output of <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> is similar to the output of the RL method. Some noisy artifacts are detected in areas with low magnetic-field strength.</p>
<p>A detailed analysis can reveal some differences however. In the light–bridge (LB), the magnetic field is lower in the RL version. Additionally, the polarity inversion line (PIL) appears sharper and broader in the RL version than in the <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> one. The magnetic flux in both areas (LB and PIL) is reduced by a factor 0.5, which might be an indication of
too many iterations. The magnetic field strength of the umbra is between 50G and 80G higher in the RL version.</p>
<p>As a final test, we checked the difference between the original image and the output of <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> convolved with the PSF. The average relative difference is around 4% (which is in the range 10-80G depending on the flux of the pixel),
which goes down to less than 1% in the RL case (this is a clear indication that <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> is introducing prior information not present in the data). Additionally, our network is orders of magnitude faster than RL, it does not create  noisy artifacts and the estimation of the magnetic field is as robust as a RL method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rlucy</span><span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">psf</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">damped</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; working on it&quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">astropy.convolution</span> <span class="kn">import</span> <span class="n">convolve_fft</span> <span class="k">as</span> <span class="n">convolve</span>

    <span class="n">psf</span> <span class="o">/=</span> <span class="n">psf</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">lucy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">raw</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">raw</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niter</span><span class="p">):</span>
        <span class="n">ratio</span> <span class="o">=</span> <span class="n">raw</span> <span class="o">/</span> <span class="n">convolve</span><span class="p">(</span><span class="n">lucy</span><span class="p">,</span> <span class="n">psf</span><span class="p">,</span> <span class="n">boundary</span><span class="o">=</span><span class="s1">&#39;wrap&#39;</span><span class="p">)</span>
        <span class="n">ratio</span><span class="p">[</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">ratio</span><span class="p">)</span> <span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">top</span> <span class="o">=</span> <span class="n">convolve</span><span class="p">(</span> <span class="n">ratio</span><span class="p">,</span> <span class="n">psf</span><span class="p">,</span> <span class="n">boundary</span><span class="o">=</span><span class="s1">&#39;wrap&#39;</span><span class="p">)</span>
        <span class="n">top</span><span class="p">[</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">top</span><span class="p">)</span> <span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">lucy</span> <span class="o">=</span> <span class="n">lucy</span> <span class="o">*</span> <span class="n">top</span> 
        <span class="n">dife</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">convolve</span><span class="p">(</span><span class="n">lucy</span><span class="p">,</span> <span class="n">psf</span><span class="p">,</span> <span class="n">boundary</span><span class="o">=</span><span class="s1">&#39;wrap&#39;</span><span class="p">)</span> <span class="o">-</span> <span class="n">raw</span><span class="p">)</span>
        <span class="n">chisq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanmedian</span><span class="p">(</span><span class="n">dife</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span><span class="n">chisq</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="n">damped</span><span class="p">:</span>
            <span class="n">umbral</span> <span class="o">=</span> <span class="n">chisq</span><span class="o">*</span><span class="mf">10.0</span>
            <span class="n">lucy</span><span class="p">[</span><span class="n">dife</span><span class="o">&gt;</span><span class="n">umbral</span><span class="p">]</span> <span class="o">=</span> <span class="n">raw</span><span class="p">[</span><span class="n">dife</span><span class="o">&gt;</span><span class="n">umbral</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">lucy</span>


<span class="k">def</span> <span class="nf">new_PSF_scaled</span><span class="p">():</span>
    <span class="c1"># We refill the Airy PSF created by astropy with our new values</span>
    <span class="c1"># The radius of the Airy disk kernel [in pixels]</span>
    <span class="n">radio_aprx</span> <span class="o">=</span> <span class="mf">1.1</span><span class="o">/</span><span class="p">(</span><span class="mf">0.504302</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span>
    <span class="n">psfs0</span> <span class="o">=</span> <span class="n">AiryDisk2DKernel</span><span class="p">(</span><span class="n">radio_aprx</span><span class="p">)</span>

    <span class="n">psfs1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">psfs0</span><span class="p">)</span>
    <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">=</span> <span class="n">psfs0</span><span class="o">.</span><span class="n">center</span>
    <span class="k">for</span> <span class="n">ypos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">psfs1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">xpos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">psfs1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
            <span class="n">rr</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">xpos</span><span class="o">-</span><span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="o">+</span><span class="nb">abs</span><span class="p">(</span><span class="n">ypos</span><span class="o">-</span><span class="n">y0</span><span class="p">)</span><span class="o">**</span><span class="mf">2.</span><span class="p">))</span>
            <span class="n">rr_pix</span> <span class="o">=</span> <span class="n">rr</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">D</span><span class="o">/</span><span class="n">lambdai</span><span class="o">/</span><span class="mf">206265.</span><span class="o">*</span><span class="p">(</span><span class="mf">0.504302</span><span class="o">/</span><span class="mf">2.</span><span class="p">)</span>
            <span class="n">psfs1</span><span class="p">[</span><span class="n">ypos</span><span class="p">,</span><span class="n">xpos</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_PSF</span><span class="p">(</span><span class="n">rr_pix</span><span class="p">)</span>
    <span class="n">psfs1</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">psfs1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">psfs1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># As we will deconvolve the interpolated version of the </span>
<span class="c1"># magnetogram, we also need to scale the PSF</span>
<span class="n">psfHMI</span> <span class="o">=</span> <span class="n">new_PSF_scaled</span><span class="p">()</span>

<span class="c1"># We load the magnetograms in kG</span>
<span class="n">pHMI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;blos_paper0.npy&#39;</span><span class="p">)</span><span class="o">*</span><span class="mf">1000.</span>
<span class="n">pEhnhace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;blos_paper1.npy&#39;</span><span class="p">)</span><span class="o">*</span><span class="mf">1000.</span>


<span class="c1"># NOTE: we have implemented a damping method to avoid very large values.</span>
<span class="c1"># It can be enable/disabled using damped=True/False in this function:</span>
<span class="n">pLucy</span> <span class="o">=</span> <span class="n">rlucy</span><span class="p">(</span><span class="n">pHMI</span><span class="p">,</span> <span class="n">psf</span><span class="o">=</span><span class="n">psfHMI</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">damped</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mf">3.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.size&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">8</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pEhnhace</span><span class="p">,</span><span class="n">vmin</span><span class="o">=-</span><span class="mi">1500</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Y [pixel]&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pLucy</span><span class="p">,</span><span class="n">vmin</span><span class="o">=-</span><span class="mi">1500</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span><span class="n">labelleft</span><span class="o">=</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">pLucy</span><span class="o">-</span><span class="n">pEhnhace</span><span class="p">,</span><span class="n">vmin</span><span class="o">=-</span><span class="mi">200</span><span class="p">,</span><span class="n">vmax</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span><span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span><span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;None&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X [pixel]&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">tick_params</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span><span class="n">labelleft</span><span class="o">=</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/notebook_69_0.png" src="../_images/notebook_69_0.png" />
</div>
</div>
<p><a id="fig:compara2"></a>
<strong>Figure 11</strong> — Left: the output of Enhance. Middle: output after applying a Richardson-Lucy method to deconvolve the image. Right: the difference between the RL version and the Enhance output. The magnetic flux has been clipped to ±1.5kG and ±200G in the last image.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="conclusions-and-future-work">
<h2>Conclusions and future work<a class="headerlink" href="#conclusions-and-future-work" title="Permalink to this headline">¶</a></h2>
<p>This paper presents the first successful deconvolution and super-resolution applied on solar images using deep convolutional neural network. It represents, after <a class="reference external" href="#references">Asensio Ramos et al. (2017)</a>, a new step toward the implementation of new machine learning techniques in the field of solar physics.</p>
<p>Single-image super-resolution and deconvolution, either for continuum images or for magnetograms, is an ill-defined problem. It requires the addition of extra knowledge for what to expect in the high-resolution images. The deep learning approach presented in this paper extracts this knowledge from the simulations and also applies a deconvolution. All this
is done very quickly, almost in real-time, and to images of arbitrary size. We hope that <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> will allow researchers to study small-scale details in HMI images and magnetograms, something that is currently impossible.</p>
<p>Often, HMI is used not as the primary source of information but as a complement for ground-based observations, providing the context. For this reason, having enhanced images where you can analyze the context with increased resolution is interesting.</p>
<p>We have preferred to be conservative and only do super-resolution by a factor two. We carried out some tests with a larger factor, but the results were not satisfactory. Whether or not other techniques proposed in this explosively growing field can work better remains to be tested. Among others, techniques like a gradual up-sampling  <a class="reference external" href="#references">(Zhao et al. 2017)</a>, recursive convolutional layers <a class="reference external" href="#references">(Kim et al. 2015)</a>, recursive residual blocks <a class="reference external" href="#references">(Tai et al. 2017)</a>, or using adversarial networks as a more elaborate loss function <a class="reference external" href="#references">(Ledig et al. 2016; Schawinski et al. 2017)</a> could potentially produce better results.</p>
<p>We provide <code class="docutils literal notranslate"><span class="pre">Enhance</span></code> hosted on https://github.com/cdiazbas/enhance as an open-source tool, providing the
methods to apply the trained networks used in this work to HMI images or re-train them using new data. In the future, we plan to extend the technique to other telescopes/instruments to generate super-resolved and deconvolved images.</p>
<hr class="docutils" />
<p><em>We would like to thank Monica Bobra and her collaborators for promoting these new methods of analysis to be used in solar physics and for inviting us to do it with this contribution.</em></p>
<p><em>As this chapter is based on a publication in A&amp;A, 614, A5 (2018) we also like to thank the anonymous referee of our article. We thank Mark Cheung for kindly sharing with us the simulation data, without which this study would not have been possible.</em></p>
<p><em>Financial support by the Spanish Ministry of Economy and Competitiveness through project AYA2014-60476-P is gratefully acknowledged. CJDB acknowledges Fundaci’on La Caixa for the financial support received in the form of a PhD contract.</em></p>
<p><em>We also thank the NVIDIA Corporation for the donation of the Titan X GPU used in this research. This research has made use of NASA’s Astrophysics Data System Bibliographic Services.</em></p>
<p><em>We acknowledge the community effort devoted to the development of the following open-source packages that were used in this work: numpy (numpy.org), matplotlib (matplotlib.org), Keras (keras.io), Tensorflow (tensorflow.org)
and SunPy (sunpy.org).</em></p>
<hr class="docutils" />
</div>
</div>
<hr class="docutils" />
<div class="section" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h1>
<p><a id="references"></a></p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.aanda.org/articles/aa/abs/2015/05/aa25508-14/aa25508-14.html">Asensio Ramos, A. &amp; de la Cruz Rodríguez, J. 2015, A&amp;A, 577, A140</a></p></li>
<li><p><a class="reference external" href="https://www.aanda.org/articles/aa/abs/2017/08/aa30783-17/aa30783-17.html">Asensio Ramos, A., Requerey, I. S., &amp; Vitas, N. 2017, A&amp;A, 604, A11</a></p></li>
<li><p><a class="reference external" href="https://www.aanda.org/articles/aa/abs/2005/30/aa2865-05/aa2865-05.html">Asensio Ramos, A. &amp; Socas-Navarro, H. 2005, A&amp;A, 438, 1021</a></p></li>
<li><p><a class="reference external" href="https://academic.oup.com/pasj/article/66/SP1/S16/1459365">Bamba, Y., Kusano, K., Imada, S., &amp; Iida, Y. 2014, PASJ, 66, S16</a></p></li>
<li><p><a class="reference external" href="https://www.aanda.org/component/article?access=bibcode&amp;bibcode=&amp;bibcode=2009A%2526A...494.1091BFUL">Bello González, N., Yelles Chaouche, L., Okunev, O., &amp; Kneer, F. 2009, A&amp;A, 494, 1091</a></p></li>
<li><p><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.679.1104&amp;rep=rep1&amp;type=pdf">Bishop, C. M. 1996, Neural networks for pattern recognition (Oxford University Press)</a></p></li>
<li><p><a class="reference external" href="http://www.seanborman.com/publications/mwscas98.pdf">Borman, S. &amp; Stevenson, R. L. 1998, Midwest Symposium on Circuits and Systems, 374</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2008A%26A...481L..37C">Carroll, T. A. &amp; Kopf, M. 2008, A&amp;A, 481, L37</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2010ApJ...720..233C">Cheung, M. C. M., Rempel, M., Title, A. M., &amp; Schüssler, M. 2010, ApJ, 720, 233</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1708.08878">Ciuca, R., Hernández, O. F., &amp; Wolman, M. 2017, ArXiv e-prints arXiv:1708.08878</a></p></li>
<li><p><a class="reference external" href="https://link.springer.com/article/10.1007%2Fs11207-007-9094-3">Colak, T. &amp; Qahwaji, R. 2008, Sol. Phys., 248, 277</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2008SoPh..248..277C">Colak, T., &amp; Qahwaji, R. 2008, Sol. Phys., 248, 277</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2016SoPh..291.1887C">Couvidat, S., Schou, J., Hoeksema, J. T., et al. 2016, Sol. Phys., 291, 1887</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2008A%26A...484L..17D">Danilovic, S., Gandorfer, A., Lagg, A., et al. 2008, A&amp;A, 484, L17</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2010A%26A...513A...1D">Danilovic, S., Schüssler, M., &amp; Solanki, S. K. 2010, A&amp;A, 513, A1</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2015ApJ...811..107D">DeRosa, M. L., Wheatland, M. S., Leka, K. D., et al. 2015, ApJ, 811, 107</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1501.00092">Dong, C., Change Loy, C., He, K., &amp; Tang, X. 2015, ArXiv e-prints arXiv:1501.00092</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1608.00367">Dong, C., Change Loy, C., &amp; Tang, X. 2016, ArXiv e-prints arXiv:1608.00367</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2017arXiv170609077H">Hayat, K. 2017, ArXiv e-prints arXiv:1706.09077</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1512.03385">He, K., Zhang, X., Ren, S., &amp; Sun, J. 2015, ArXiv e-prints arXiv:1512.03385</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2008SoPh..249..233I">Ichimoto, K., Lites, B., Elmore, D., et al. 2008, Sol. Phys., 249, 233</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1502.03167">Ioffe, S., &amp; Szegedy, C. 2015, ICML-15, eds. D. Blei, &amp; F. Bach, 448</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2015arXiv151104491K">Kim, J., Lee, J. K., &amp; Lee, K. M. 2015, ArXiv e-prints arXiv:1511.04491</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2014arXiv1412.6980K">Kingma, D. P., &amp; Ba, J. 2014, ArXiv e-prints arXiv:1412.6980</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2007SoPh..243....3K">Kosugi, T., Matsuzaki, K., Sakao, T., et al. 2007, Sol. Phys., 243, 3</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2004A%26A...417.1125K">Krivova, N. A., &amp; Solanki, S. K. 2004, A&amp;A, 417, 1125</a></p></li>
<li><p><a class="reference external" href="http://dl.acm.org/citation.cfm?id=303568.303704">LeCun, Y., &amp; Bengio, Y. 1998, ed. M. A. Arbib (Cambridge, MA: MIT Press), 255</a></p></li>
<li><p><a class="reference external" href="http://dl.acm.org/citation.cfm?id=645754.668382">LeCun, Y., Bottou, L., Orr, G. B., &amp; Müller, K.-R. 1998, NIPS Workshop (London, UK: Springer-Verlag), 9</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1609.04802">Ledig, C., Theis, L., Huszar, F., et al. 2016, ArXiv e-prints arXiv:1609.04802</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2017arXiv170802342L">Linker, J. A., Caplan, R. M., Downs, C., et al. 2017, ApJ, 848, 70</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2013SoPh..283..579L">Lites, B. W., Akin, D. L., Card, G., et al. 2013, Sol. Phys., 283, 579</a></p></li>
<li><p><a class="reference external" href="http://www.icml2010.org/papers/432.pdf">Nair, V., &amp; Hinton, G. E. 2010, ICML-10, (Haïfa: ACM Digital Library), 21, 807</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2012SoPh..275....3P">Pesnell, W. D., Thompson, B. J., &amp; Chamberlin, P. C. 2012, Sol. Phys., 275, 3</a></p></li>
<li><p><a class="reference external" href="http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220%2f0005297200840091">Peyrard, C., Mamalet, F., &amp; Garcia, C. 2015, in VISAPP, eds. J. Braz, S. Battiato, &amp; J. F. H. Imai (Setùbal: SciTePress), 1, 84</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2013SoPh..282...91P">Pietarila, A., Bertello, L., Harvey, J. W., &amp; Pevtsov, A. A. 2013, Sol. Phys., 282, 91</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2015A%26A...579A...3Q">Quintero Noda, C., Asensio Ramos, A., Orozco Suárez, D., &amp; Ruiz Cobo B. 2015, A&amp;A, 579, A3</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/1972JOSA...62...55R">Richardson, W. H. 1972, J. Opt. Soc. Am, 62, 55</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2013A%26A...549L...4R">Ruiz Cobo, B., &amp; Asensio Ramos A. 2013, A&amp;A, 549, L4</a></p></li>
<li><p><a class="reference external" href="http://dl.acm.org/citation.cfm?id=65669.104451">Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. 1986, (Cambridge, MA: MIT Press), Nature, 323, 533</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2017MNRAS.467L.110S">Schawinski, K., Zhang, C., Zhang, H., Fowler, L., &amp; Santhanam, G. K. 2017, MNRAS, 467, L110</a></p></li>
<li><p><a class="reference external" href="http://dx.doi.org/10.1007/s11207-011-9834-2">Scherrer, P. H., Schou, J., Bush, R. I., et al. 2012, Sol. Phys., 275, 207</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2014arXiv1404.7828S">Schmidhuber, J. 2015, Neural Networks, 61, 85</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2016arXiv160905158S">Shi, W., Caballero, J., Huszár, F., et al. 2016, ArXiv e-prints arXiv:1609.05158</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2014arXiv1409.1556S">Simonyan, K., &amp; Zisserman, A. 2014, ArXiv e-prints arXiv:1409.1556</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2005ApJ...621..545S">Socas-Navarro, H. 2005, ApJ, 621, 545</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2012LRSP....9....4S">Stein, R. F. 2012, Liv. Rev. Sol. Phys., 9, 4</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2012LRSP....9....4S">Stein, R. F., &amp; Nordlund, Å. 2012, ApJ, 753, L13</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2013A%26A...550A..14T">Tadesse, T., Wiegelmann, T., Inhester, B., et al. 2013, A&amp;A, 550, A14</a></p></li>
<li><p><a class="reference external" href="https://ieeexplore.ieee.org/document/8099781/">Tai, Y., Yang, J., &amp; Liu, X. 2017, Proceeding of IEEE Computer Vision and Pattern Recognition</a></p></li>
<li><p><a class="reference external" href="https://papers.nips.cc/paper/2315-bayesian-image-super-resolution.pdf">Tipping, M. E., &amp; Bishop, C. M. 2003, (Cambridge, MA: MIT Press), 1303</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2008SoPh..249..167T">Tsuneta, S., Ichimoto, K., Katsukawa, Y., et al. 2008, Sol. Phys., 249, 167</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2012A%26A...548A...5V">van Noort, M. 2012, A&amp;A, 548, A5</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2005A%26A...429..335V">Vögler, A., Shelyag, S., Schüssler, M., et al. 2005, A&amp;A, 429, 335</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2012SoPh..275..261W">Wachter, R., Schou, J., Rabello-Soares, M. C., et al. 2012, Sol. Phys., 275, 261</a></p></li>
<li><p><a class="reference external" href="http://dl.acm.org/citation.cfm?id=2968826.2969026">Xu, L., Ren, J. S. J., Liu, C., &amp; Jia, J. 2014, NIPS’14 (Cambridge, MA: MIT Press), 1790</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2014A%26A...561A..22Y">Yeo, K. L., Feller, A., Solanki, S. K., et al. 2014, A&amp;A, 561, A22</a></p></li>
<li><p><a class="reference external" href="http://adsabs.harvard.edu/abs/2017arXiv170304244Z">Zhao, Y., Wang, R., Dong, W., et al. 2017, ArXiv e-prints arXiv:1703.04244</a></p></li>
</ul>
<hr class="docutils" />
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./03"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="Enhancing_SDO_images.html" title="previous page">Enhancing SDO Images</a>
    <a class='right-next' id="next-link" href="../04/Differential_Emission_Measurements.html" title="next page">Differential Emission Measurements</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Monica Bobra and James Mason<br/>
        
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>